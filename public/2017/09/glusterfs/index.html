<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>GlusterFS - smcleod.net</title><meta name=Description content><meta property="og:title" content="GlusterFS"><meta property="og:description" content="We&rsquo;re in the process of shifting from using our custom &lsquo;glue&rsquo; for orchestrating Docker deployments to Kubernetes, When we first deployed Docker to replace LXC and our legacy Puppet-heavy application configuration and deployment systems there really wasn&rsquo;t any existing tool to manage this, thus we rolled our own, mainly a few Ruby scripts combined with a Puppet / Hiera / Mcollective driven workflow.
The main objective is to replace our legacy NFS file servers used to host uploads / attachments and static files for our web applications, while NFS(v4) performance is adequate, it is a clear single point of failure and of course, there are the age old stale mount problems should network interruptions occur."><meta property="og:type" content="article"><meta property="og:url" content="https://beta.smcleod.net/2017/09/glusterfs/"><meta property="og:image" content="https://beta.smcleod.net/apple-touch-icon.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-09-25T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-31T15:04:28+11:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://beta.smcleod.net/apple-touch-icon.png"><meta name=twitter:title content="GlusterFS"><meta name=twitter:description content="We&rsquo;re in the process of shifting from using our custom &lsquo;glue&rsquo; for orchestrating Docker deployments to Kubernetes, When we first deployed Docker to replace LXC and our legacy Puppet-heavy application configuration and deployment systems there really wasn&rsquo;t any existing tool to manage this, thus we rolled our own, mainly a few Ruby scripts combined with a Puppet / Hiera / Mcollective driven workflow.
The main objective is to replace our legacy NFS file servers used to host uploads / attachments and static files for our web applications, while NFS(v4) performance is adequate, it is a clear single point of failure and of course, there are the age old stale mount problems should network interruptions occur."><meta name=application-name content="smcleod.net"><meta name=apple-mobile-web-app-title content="smcleod.net"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=../../../logo.svg><link rel=apple-touch-icon sizes=180x180 href=../../../apple-touch-icon.png><link rel=mask-icon href=../../../safari-pinned-tab.svg color=#5bbad5><link rel=canonical href=https://beta.smcleod.net/2017/09/glusterfs/><link rel=prev href=https://beta.smcleod.net/2017/09/return-of-the-rss/><link rel=next href=https://beta.smcleod.net/2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet/><link rel=stylesheet href=../../../lib/normalize/normalize.min.8235a67a2521ef99fb1e455a5891b022af1653b4ede3cfffcac4e473beee88fbcc1a36bbb3cfdd75fd6df46732032f9d96e30adae9c88b769e1fbf7945cd27e0.css integrity="sha512-gjWmeiUh75n7HkVaWJGwIq8WU7Tt48//ysTkc77uiPvMGja7s8/ddf1t9GcyAy+dluMK2unIi3aeH795Rc0n4A=="><link rel=stylesheet href=../../../css/style.min.18108d84e024a889c14d785ff688fe13b654fa9bdc1118490c01fd2d42d6d7ae133fc32040282141aa703a4581fc715573ddde9fdb5f6520fbace1c06f846cd0.css integrity="sha512-GBCNhOAkqInBTXhf9oj+E7ZU+pvcERhJDAH9LULW164TP8MgQCghQapwOkWB/HFVc93en9tfZSD7rOHAb4Rs0A=="><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=../../../lib/fontawesome-free/all.min.eb8387c9eb0ab6d4fa4d7ad397d15df13b92c009a1eb221b757dd1ecfccabc78a4c5b681847a7f4198ff7cd60abed8300508c9dea138cd1f4a85f09f09b2092a.css integrity="sha512-64OHyesKttT6TXrTl9Fd8TuSwAmh6yIbdX3R7PzKvHikxbaBhHp/QZj/fNYKvtgwBQjJ3qE4zR9KhfCfCbIJKg=="><noscript><link rel=stylesheet href=../../../lib/fontawesome-free/all.min.eb8387c9eb0ab6d4fa4d7ad397d15df13b92c009a1eb221b757dd1ecfccabc78a4c5b681847a7f4198ff7cd60abed8300508c9dea138cd1f4a85f09f09b2092a.css integrity="sha512-64OHyesKttT6TXrTl9Fd8TuSwAmh6yIbdX3R7PzKvHikxbaBhHp/QZj/fNYKvtgwBQjJ3qE4zR9KhfCfCbIJKg=="></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=../../../lib/animate/animate.min.738daa4d2c3fc0f677ff92c1cc3f81c397fb6d2176a31a2eeb011bf88fe5a9e68a57914321f32fbd1a7bef6cb88dc24b2ae1943a96c931d83f053979d1f25803.css integrity="sha512-c42qTSw/wPZ3/5LBzD+Bw5f7bSF2oxou6wEb+I/lqeaKV5FDIfMvvRp772y4jcJLKuGUOpbJMdg/BTl50fJYAw=="><noscript><link rel=stylesheet href=../../../lib/animate/animate.min.738daa4d2c3fc0f677ff92c1cc3f81c397fb6d2176a31a2eeb011bf88fe5a9e68a57914321f32fbd1a7bef6cb88dc24b2ae1943a96c931d83f053979d1f25803.css integrity="sha512-c42qTSw/wPZ3/5LBzD+Bw5f7bSF2oxou6wEb+I/lqeaKV5FDIfMvvRp772y4jcJLKuGUOpbJMdg/BTl50fJYAw=="></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"GlusterFS","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/beta.smcleod.net\/2017\/09\/glusterfs\/"},"image":["https:\/\/beta.smcleod.net\/apple-touch-icon.png"],"genre":"posts","keywords":"tech, storage, software","wordcount":1094,"url":"https:\/\/beta.smcleod.net\/2017\/09\/glusterfs\/","datePublished":"2017-09-25T00:00:00+00:00","dateModified":"2022-10-31T15:04:28+11:00","license":"Sam McLeod","publisher":{"@type":"Organization","name":"Sam McLeod"},"author":{"@type":"Person","name":"Sam McLeod"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark")}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=../../../ title=smcleod.net><img class="lazyload logo" data-src=../../../logo.svg data-srcset="../../../logo.svg, ../../../logo.svg 1.5x, ../../../logo.svg 2x" data-sizes=auto alt=/logo.svg title=/logo.svg>smcleod.net</a></div><div class=menu><div class=menu-inner><a class=menu-item href=../../../posts/>üìù Posts </a><a class=menu-item href=../../../categories/>üç± Categories </a><a class=menu-item href=../../../series/>üìö Series </a><a class=menu-item href=../../../links/ title="Links to software and services I use">üîó Links </a><a class=menu-item href=../../../about/ title="Sam McLeod">ü§¶üèª‚Äç‚ôÇÔ∏è About </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span>
</span><a href=# onclick=return!1 class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=../../../ title=smcleod.net><img class="lazyload logo" data-src=../../../logo.svg data-srcset="../../../logo.svg, ../../../logo.svg 1.5x, ../../../logo.svg 2x" data-sizes=auto alt=/logo.svg title=/logo.svg>smcleod.net</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=# onclick=return!1 class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=../../../posts/ title>üìù Posts</a><a class=menu-item href=../../../categories/ title>üç± Categories</a><a class=menu-item href=../../../series/ title>üìö Series</a><a class=menu-item href=../../../links/ title="Links to software and services I use">üîó Links</a><a class=menu-item href=../../../about/ title="Sam McLeod">ü§¶üèª‚Äç‚ôÇÔ∏è About</a><a href=# onclick=return!1 class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class="toc-content always-active" id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#implementation>Implementation</a></li><li><a href=#automation--puppet>Automation / Puppet</a></li><li><a href=#performance>Performance</a><ul><li><a href=#sequential-read>Sequential Read</a></li><li><a href=#sequential-write>Sequential Write</a></li><li><a href=#4k-write-iops>4K Write IOP/s</a></li><li><a href=#4k-read-iops>4K Read IOP/s</a></li></ul></li><li><a href=#annoyances>Annoyances</a><ul><li><a href=#open-files>Open Files</a></li><li><a href=#inconsistent-configuration>Inconsistent Configuration</a></li><li><a href=#memory-usage--leaks>Memory Usage & Leaks</a></li><li><a href=#poor-gluster-cli-performance>Poor Gluster CLI Performance</a></li><li><a href=#broken-links-to-glusterorg-and-glusterreadthedocsio>Broken Links to Gluster.org and Gluster.readthedocs.io</a></li></ul></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">GlusterFS</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=https://github.com/sammcj title=Author target=_blank rel="noopener noreferrer author" class=author>Sam McLeod</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=../../../categories/tech/><i class="far fa-folder fa-fw"></i>Tech</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2017-09-25>2017-09-25</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-10-31>2022-10-31</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1094 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;6 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#implementation>Implementation</a></li><li><a href=#automation--puppet>Automation / Puppet</a></li><li><a href=#performance>Performance</a><ul><li><a href=#sequential-read>Sequential Read</a></li><li><a href=#sequential-write>Sequential Write</a></li><li><a href=#4k-write-iops>4K Write IOP/s</a></li><li><a href=#4k-read-iops>4K Read IOP/s</a></li></ul></li><li><a href=#annoyances>Annoyances</a><ul><li><a href=#open-files>Open Files</a></li><li><a href=#inconsistent-configuration>Inconsistent Configuration</a></li><li><a href=#memory-usage--leaks>Memory Usage & Leaks</a></li><li><a href=#poor-gluster-cli-performance>Poor Gluster CLI Performance</a></li><li><a href=#broken-links-to-glusterorg-and-glusterreadthedocsio>Broken Links to Gluster.org and Gluster.readthedocs.io</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>We&rsquo;re in the process of shifting from using our custom &lsquo;glue&rsquo; for orchestrating Docker deployments to Kubernetes, When we first deployed Docker to replace LXC and our legacy Puppet-heavy application configuration and deployment systems there really wasn&rsquo;t any existing tool to manage this, thus we rolled our own, mainly a few Ruby scripts combined with a Puppet / Hiera / Mcollective driven workflow.</p><p>The main objective is to replace our legacy NFS file servers used to host uploads / attachments and static files for our web applications, while NFS(v4) performance is adequate, it is a clear single point of failure and of course, there are the age old stale mount problems should network interruptions occur.</p><p>I spent time evaluating various cluster filesystems / network block storage and the two that stood out were Ceph and Gluster and settled on Gluster as the most suitable for our needs, it&rsquo;s far less complex to deploy than Ceph, it has less moving pieces and files are stored in a familiar manner on hosts.</p><h2 id=implementation class=headerLink><a href=#implementation class=header-mark></a>Implementation</h2><p>I&rsquo;ve settled on a 3 node deployment with one node as an <a href=http://docs.gluster.org/en/latest/Administrator%20Guide/arbiter-volumes-and-quorum/ target=_blank rel="noopener noreferrer">arbiter</a> (replica 3, arbiter 1).</p><p>Our nodes are CentOS 7 VMs within our exiting XenServer infrastructure, each node has 8 vCPUs <a href=https://ark.intel.com/products/91754/Intel-Xeon-Processor-E5-2680-v4-35M-Cache-2_40-GHz target=_blank rel="noopener noreferrer">Xeon E5-2680 v4</a>, 16GB of RAM and backed by our <a href=https://smcleod.net/tech/ssd-storage-cluster-diagram/ target=_blank rel="noopener noreferrer">iSCSI SSD Storage</a>.</p><h2 id=automation--puppet class=headerLink><a href=#automation--puppet class=header-mark></a>Automation / Puppet</h2><p>We&rsquo;re long time, heavy users of Puppet so naturally I&rsquo;m deploying Gluster via a <a href=https://github.com/voxpupuli/puppet-gluster target=_blank rel="noopener noreferrer">Puppet module</a> and generating volumes and volume configuration from <a href=https://docs.puppet.com/hiera/ target=_blank rel="noopener noreferrer">Hiera</a>.</p><p>Volumes are automatically generated from the Hiera structure that defines our applications.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># profiles::services::gluster::volume:</span>
</span></span><span class=line><span class=cl>profiles::services::gluster::volume::pool: <span class=s2>&#34;%{alias(&#39;profiles::services::gluster::host::pool&#39;)}&#34;</span>
</span></span><span class=line><span class=cl>profiles::services::gluster::volume::pool_members: <span class=s2>&#34;%{alias(&#39;profiles::services::gluster::host::pool_members&#39;)}&#34;</span>
</span></span><span class=line><span class=cl>profiles::services::gluster::volume::brick_mountpoint: <span class=s1>&#39;/mnt/gluster-storage&#39;</span>
</span></span><span class=line><span class=cl>profiles::services::gluster::volume::replica: <span class=m>3</span>
</span></span><span class=line><span class=cl>profiles::services::gluster::volume::arbiter: <span class=m>1</span>
</span></span><span class=line><span class=cl>profiles::services::gluster::volume::volume_options:
</span></span><span class=line><span class=cl>  <span class=c1># Failover clients after 10 seconds of a server being unavailable</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;network.ping-timeout&#39;</span>: <span class=s1>&#39;10&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;cluster.lookup-optimize&#39;</span>: <span class=s1>&#39;true&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;cluster.readdir-optimize&#39;</span>: <span class=s1>&#39;true&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;cluster.use-compound-fops&#39;</span>: <span class=s1>&#39;true&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;performance.parallel-readdir&#39;</span>: <span class=s1>&#39;true&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;performance.client-io-threads&#39;</span>: <span class=s1>&#39;true&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;performance.stat-prefetch&#39;</span>: <span class=s1>&#39;true&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;diagnostics.brick-log-level&#39;</span>: <span class=s1>&#39;WARNING&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;diagnostics.client-log-level&#39;</span>: <span class=s1>&#39;WARNING&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;server.event-threads&#39;</span>: <span class=s1>&#39;3&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;client.event-threads&#39;</span>: <span class=s1>&#39;3&#39;</span>
</span></span></code></pre></div><p>I enabled the <code>nis_enabled</code> SEbool to prevent a number of SELinux denials I noticed in the logs:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>profiles::os::redhat::centos7::selinux::selinux_booleans:
</span></span><span class=line><span class=cl>  <span class=s1>&#39;nis_enabled&#39;</span>:
</span></span><span class=line><span class=cl>    value: <span class=s1>&#39;on&#39;</span>
</span></span></code></pre></div><p>I also increased the local emepheral port range and the kernel&rsquo;s socket backlog limit as suggested by Redhat:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>sysctl::base::values:
</span></span><span class=line><span class=cl>  <span class=s1>&#39;net.ipv4.tcp_max_syn_backlog&#39;</span>:
</span></span><span class=line><span class=cl>    ensure: present
</span></span><span class=line><span class=cl>    value: <span class=s1>&#39;4096&#39;</span>
</span></span><span class=line><span class=cl>    comment: <span class=s1>&#39;Increase syn backlogs for gluster&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;net.ipv4.ip_local_port_range&#39;</span>:
</span></span><span class=line><span class=cl>    ensure: present
</span></span><span class=line><span class=cl>    value: <span class=s1>&#39;32768 65535&#39;</span>
</span></span><span class=line><span class=cl>    comment: <span class=s1>&#39;Increase local port range for gluster&#39;</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;net.core.somaxconn&#39;</span>:
</span></span><span class=line><span class=cl>    ensure: present
</span></span><span class=line><span class=cl>    value: <span class=s1>&#39;2048&#39;</span>
</span></span><span class=line><span class=cl>    comment: <span class=s1>&#39;Increase kernel socket backlog limit for gluster&#39;</span>
</span></span></code></pre></div><h2 id=performance class=headerLink><a href=#performance class=header-mark></a>Performance</h2><p>Performance is, well, very poor for anything other than large reads.</p><p>I was expecting a hit to IOP/s performance as you would with any clustered, network file system, but I wasn&rsquo;t expecting it to drop as much as it did, especially after enabling the above performance options on the volumes.</p><p>![gluster perf vs native.jpg]({{site.baseurl}}/img/gluster perf vs native.jpg)</p><h3 id=sequential-read class=headerLink><a href=#sequential-read class=header-mark></a>Sequential Read</h3><p>Acceptable performance at 380MB/s:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=1M --iodepth=32 --size=5G --readwrite=read</span>
</span></span><span class=line><span class=cl>test: <span class=o>(</span><span class=nv>g</span><span class=o>=</span>0<span class=o>)</span>: <span class=nv>rw</span><span class=o>=</span>read, <span class=nv>bs</span><span class=o>=</span>1M-1M/1M-1M/1M-1M, <span class=nv>ioengine</span><span class=o>=</span>libaio, <span class=nv>iodepth</span><span class=o>=</span><span class=m>32</span>
</span></span><span class=line><span class=cl>fio-2.1.11
</span></span><span class=line><span class=cl>Starting <span class=m>1</span> process
</span></span><span class=line><span class=cl>Jobs: <span class=m>1</span> <span class=o>(</span><span class=nv>f</span><span class=o>=</span>1<span class=o>)</span>: <span class=o>[</span>R<span class=o>(</span>1<span class=o>)]</span> <span class=o>[</span>100.0% <span class=k>done</span><span class=o>]</span> <span class=o>[</span>380.0MB/0KB/0KB /s<span class=o>]</span> <span class=o>[</span>380/0/0 iops<span class=o>]</span> <span class=o>[</span>eta 00m:00s<span class=o>]</span>
</span></span><span class=line><span class=cl>test: <span class=o>(</span><span class=nv>groupid</span><span class=o>=</span>0, <span class=nv>jobs</span><span class=o>=</span>1<span class=o>)</span>: <span class=nv>err</span><span class=o>=</span> 0: <span class=nv>pid</span><span class=o>=</span>509: Mon Sep <span class=m>25</span> 12:17:06 <span class=m>2017</span>
</span></span><span class=line><span class=cl>  <span class=nb>read</span> : <span class=nv>io</span><span class=o>=</span>5120.0MB, <span class=nv>bw</span><span class=o>=</span>310321KB/s, <span class=nv>iops</span><span class=o>=</span>303, <span class=nv>runt</span><span class=o>=</span> 16895msec
</span></span><span class=line><span class=cl>  cpu          : <span class=nv>usr</span><span class=o>=</span>0.58%, <span class=nv>sys</span><span class=o>=</span>3.50%, <span class=nv>ctx</span><span class=o>=</span>16068, <span class=nv>majf</span><span class=o>=</span>0, <span class=nv>minf</span><span class=o>=</span><span class=m>533</span>
</span></span><span class=line><span class=cl>  IO depths    : <span class=nv>1</span><span class=o>=</span>0.1%, <span class=nv>2</span><span class=o>=</span>0.1%, <span class=nv>4</span><span class=o>=</span>0.1%, <span class=nv>8</span><span class=o>=</span>0.2%, <span class=nv>16</span><span class=o>=</span>0.3%, <span class=nv>32</span><span class=o>=</span>99.4%, &gt;<span class=o>=</span><span class=nv>64</span><span class=o>=</span>0.0%
</span></span><span class=line><span class=cl>     submit    : <span class=nv>0</span><span class=o>=</span>0.0%, <span class=nv>4</span><span class=o>=</span>100.0%, <span class=nv>8</span><span class=o>=</span>0.0%, <span class=nv>16</span><span class=o>=</span>0.0%, <span class=nv>32</span><span class=o>=</span>0.0%, <span class=nv>64</span><span class=o>=</span>0.0%, &gt;<span class=o>=</span><span class=nv>64</span><span class=o>=</span>0.0%
</span></span><span class=line><span class=cl>     <span class=nb>complete</span>  : <span class=nv>0</span><span class=o>=</span>0.0%, <span class=nv>4</span><span class=o>=</span>100.0%, <span class=nv>8</span><span class=o>=</span>0.0%, <span class=nv>16</span><span class=o>=</span>0.0%, <span class=nv>32</span><span class=o>=</span>0.1%, <span class=nv>64</span><span class=o>=</span>0.0%, &gt;<span class=o>=</span><span class=nv>64</span><span class=o>=</span>0.0%
</span></span><span class=line><span class=cl>     issued    : <span class=nv>total</span><span class=o>=</span><span class=nv>r</span><span class=o>=</span>5120/w<span class=o>=</span>0/d<span class=o>=</span>0, <span class=nv>short</span><span class=o>=</span><span class=nv>r</span><span class=o>=</span>0/w<span class=o>=</span>0/d<span class=o>=</span><span class=m>0</span>
</span></span><span class=line><span class=cl>     latency   : <span class=nv>target</span><span class=o>=</span>0, <span class=nv>window</span><span class=o>=</span>0, <span class=nv>percentile</span><span class=o>=</span>100.00%, <span class=nv>depth</span><span class=o>=</span><span class=m>32</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Run status group <span class=m>0</span> <span class=o>(</span>all <span class=nb>jobs</span><span class=o>)</span>:
</span></span><span class=line><span class=cl>   READ: <span class=nv>io</span><span class=o>=</span>5120.0MB, <span class=nv>aggrb</span><span class=o>=</span>310321KB/s, <span class=nv>minb</span><span class=o>=</span>310321KB/s, <span class=nv>maxb</span><span class=o>=</span>310321KB/s, <span class=nv>mint</span><span class=o>=</span>16895msec, <span class=nv>maxt</span><span class=o>=</span>16895msec
</span></span></code></pre></div><h3 id=sequential-write class=headerLink><a href=#sequential-write class=header-mark></a>Sequential Write</h3><p>A terrible average of 25.6MB/s:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=1M --iodepth=32 --size=5G --readwrite=write</span>
</span></span><span class=line><span class=cl>test: <span class=o>(</span><span class=nv>g</span><span class=o>=</span>0<span class=o>)</span>: <span class=nv>rw</span><span class=o>=</span>write, <span class=nv>bs</span><span class=o>=</span>1M-1M/1M-1M/1M-1M, <span class=nv>ioengine</span><span class=o>=</span>libaio, <span class=nv>iodepth</span><span class=o>=</span><span class=m>32</span>
</span></span><span class=line><span class=cl>fio-2.1.11
</span></span><span class=line><span class=cl>Starting <span class=m>1</span> process
</span></span><span class=line><span class=cl>test: Laying out IO file<span class=o>(</span>s<span class=o>)</span> <span class=o>(</span><span class=m>1</span> file<span class=o>(</span>s<span class=o>)</span> / 5120MB<span class=o>)</span>
</span></span><span class=line><span class=cl>Jobs: <span class=m>1</span> <span class=o>(</span><span class=nv>f</span><span class=o>=</span>1<span class=o>)</span>: <span class=o>[</span>W<span class=o>(</span>1<span class=o>)]</span> <span class=o>[</span>18.9% <span class=k>done</span><span class=o>]</span> <span class=o>[</span>0KB/25600KB/0KB /s<span class=o>]</span> <span class=o>[</span>0/25/0 iops<span class=o>]</span> <span class=o>[</span>eta 01m:43s<span class=o>]</span>
</span></span></code></pre></div><h3 id=4k-write-iops class=headerLink><a href=#4k-write-iops class=header-mark></a>4K Write IOP/s</h3><p>A terrible average of 684 IOP/s:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=32 --size=5G --readwrite=randwrite</span>
</span></span><span class=line><span class=cl>test: <span class=o>(</span><span class=nv>g</span><span class=o>=</span>0<span class=o>)</span>: <span class=nv>rw</span><span class=o>=</span>randwrite, <span class=nv>bs</span><span class=o>=</span>4K-4K/4K-4K/4K-4K, <span class=nv>ioengine</span><span class=o>=</span>libaio, <span class=nv>iodepth</span><span class=o>=</span><span class=m>32</span>
</span></span><span class=line><span class=cl>fio-2.1.11
</span></span><span class=line><span class=cl>Starting <span class=m>1</span> process
</span></span><span class=line><span class=cl>Jobs: <span class=m>1</span> <span class=o>(</span><span class=nv>f</span><span class=o>=</span>1<span class=o>)</span>: <span class=o>[</span>w<span class=o>(</span>1<span class=o>)]</span> <span class=o>[</span>1.2% <span class=k>done</span><span class=o>]</span> <span class=o>[</span>0KB/2736KB/0KB /s<span class=o>]</span> <span class=o>[</span>0/684/0 iops<span class=o>]</span> <span class=o>[</span>eta 34m:53s<span class=o>]</span>
</span></span></code></pre></div><h3 id=4k-read-iops class=headerLink><a href=#4k-read-iops class=header-mark></a>4K Read IOP/s</h3><p>A terrible average of 3017 IOP/s:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=32 --size=5G --readwrite=randread</span>
</span></span><span class=line><span class=cl>test: <span class=o>(</span><span class=nv>g</span><span class=o>=</span>0<span class=o>)</span>: <span class=nv>rw</span><span class=o>=</span>randread, <span class=nv>bs</span><span class=o>=</span>4K-4K/4K-4K/4K-4K, <span class=nv>ioengine</span><span class=o>=</span>libaio, <span class=nv>iodepth</span><span class=o>=</span><span class=m>32</span>
</span></span><span class=line><span class=cl>fio-2.1.11
</span></span><span class=line><span class=cl>Starting <span class=m>1</span> process
</span></span><span class=line><span class=cl>Jobs: <span class=m>1</span> <span class=o>(</span><span class=nv>f</span><span class=o>=</span>1<span class=o>)</span>: <span class=o>[</span>r<span class=o>(</span>1<span class=o>)]</span> <span class=o>[</span>5.6% <span class=k>done</span><span class=o>]</span> <span class=o>[</span>12068KB/0KB/0KB /s<span class=o>]</span> <span class=o>[</span>3017/0/0 iops<span class=o>]</span> <span class=o>[</span>eta 07m:20s<span class=o>]</span>
</span></span></code></pre></div><h2 id=annoyances class=headerLink><a href=#annoyances class=header-mark></a>Annoyances</h2><h3 id=open-files class=headerLink><a href=#open-files class=header-mark></a>Open Files</h3><p>Gluster seems to love open file handles, on an average node in the cluster with 120 small volumes connected to 3 fuse clients and no files I often see up to 1.5 <em>million</em> open files:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>root@int-gluster-02:~  <span class=c1># lsof | wc -l</span>
</span></span><span class=line><span class=cl><span class=m>1042782</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root@int-gluster-01:~  <span class=c1># netstat -lnp|grep gluster|wc -l</span>
</span></span><span class=line><span class=cl><span class=m>111</span>
</span></span></code></pre></div><h3 id=inconsistent-configuration class=headerLink><a href=#inconsistent-configuration class=header-mark></a>Inconsistent Configuration</h3><p>A big gripe I have is with the location of cluster and volume configuration, it is kept in a number of different files and file locations, for example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>/etc/glusterfs/glusterd.vol
</span></span><span class=line><span class=cl>/var/lib/glusterd/options
</span></span><span class=line><span class=cl>/var/lib/glusterd/glusterd.info
</span></span><span class=line><span class=cl>/var/lib/glusterd/vols/simpleapp_static/quota.conf
</span></span><span class=line><span class=cl>/var/lib/glusterd/vols/simpleapp_static/trusted-simpleapp_static.tcp-fuse.vol
</span></span><span class=line><span class=cl>/var/lib/glusterd/vols/simpleapp_static/bricks/int-gluster-01<span class=se>\:</span>-mnt-gluster-storage-simpleapp_static
</span></span></code></pre></div><p>This makes managing the cluster and volumes with automation tools a bit of a pain and thus most tooling calls out to the <code>gluster</code> command line application to set and read configuration which is not ideal.</p><h3 id=memory-usage--leaks class=headerLink><a href=#memory-usage--leaks class=header-mark></a>Memory Usage & Leaks</h3><p>Along the way with Gluster from version 3.10 to 3.12 I&rsquo;ve encountered <em>many</em> memory leaks resulting in OOMs, corrupt volume configuration and often entire cluster rebuilds.</p><p>A lot of these seem to occur when making a large number of changes to volume configuration.</p><p>Often when Gluster runs out of memory and gets OOM killed, it corrupts <code>.vol</code> or <code>.info</code> files in <code>/var/lib/glusterd/vols/&lt;volumes>/</code>, this causes the daemon to fail to start, requires you to hunt through logs for mentions of incorrectly configured volumes, delete the files in question and hope that they correctly sync back from another node, if they don&rsquo;t or can&rsquo;t be synced back it seems you might have to destroy the volumes (or cluster) and recreate them before restoring your (brick) data from backups - this is not at all fun.</p><h3 id=poor-gluster-cli-performance class=headerLink><a href=#poor-gluster-cli-performance class=header-mark></a>Poor Gluster CLI Performance</h3><p>Again, because of the nature of cluster and volume configuration automation often has to call out to use the Gluster CLI tool to set or get information, this is made especially painful due to the tools performance - notably how long it takes to set / get options on volumes.</p><p>Setting a volume option on average takes around 1 second and the tool cannot set an option on multiple volumes at once.</p><p>This quickly adds up, suppose you have 200 volumes and you&rsquo;re setting 5 options on each volume, <code>200*5=1000 / 60</code> - that&rsquo;s <em>16.6 minutes just to set the options!</em>, this could be a real issue when recovering from a disaster scenario.</p><h3 id=broken-links-to-glusterorg-and-glusterreadthedocsio class=headerLink><a href=#broken-links-to-glusterorg-and-glusterreadthedocsio class=header-mark></a>Broken Links to Gluster.org and Gluster.readthedocs.io</h3><p>Many times I&rsquo;ve clicked on links to Gluster articles or documentation and have found them to be broken, it seems that Gluster.org has undergone changes and has not created redirects for existing permalinks.</p></div><h2>Related Content</h2><div class=related-container><div class=related-item-container><div class=related-image><a href=../../../2022/10/making-work-visible-avoid-dms/><img class=lazyload data-src=../../../2022/10/making-work-visible-avoid-dms/dm-disruption.jpg data-srcset="../../../2022/10/making-work-visible-avoid-dms/dm-disruption.jpg, ../../../2022/10/making-work-visible-avoid-dms/dm-disruption.jpg 1.5x, ../../../2022/10/making-work-visible-avoid-dms/dm-disruption.jpg 2x" data-sizes=auto alt=/2022/10/making-work-visible-avoid-dms/dm-disruption.jpg title=/2022/10/making-work-visible-avoid-dms/dm-disruption.jpg height=200 width=400></a></div><h2 class=related-title><a href=../../../2022/10/making-work-visible-avoid-dms/>Making Work Visible - Avoid DMs</a></h2></div><div class=related-item-container><div class=related-image><a href=../../../2022/10/the-best-of-2022-edition/><img class=lazyload data-src=../../../2022/10/the-best-of-2022-edition/tools.jpg data-srcset="../../../2022/10/the-best-of-2022-edition/tools.jpg, ../../../2022/10/the-best-of-2022-edition/tools.jpg 1.5x, ../../../2022/10/the-best-of-2022-edition/tools.jpg 2x" data-sizes=auto alt=/2022/10/the-best-of-2022-edition/tools.jpg title=/2022/10/the-best-of-2022-edition/tools.jpg height=200 width=400></a></div><h2 class=related-title><a href=../../../2022/10/the-best-of-2022-edition/>The Best Of - 2022 Edition</a></h2></div><div class=related-item-container><div class=related-image><a href=../../../2022/05/firefox-addons-for-2022/><img class=lazyload data-src=../../../2022/05/firefox-addons-for-2022/firefox-logo-collage-1.png data-srcset="../../../2022/05/firefox-addons-for-2022/firefox-logo-collage-1.png, ../../../2022/05/firefox-addons-for-2022/firefox-logo-collage-1.png 1.5x, ../../../2022/05/firefox-addons-for-2022/firefox-logo-collage-1.png 2x" data-sizes=auto alt=/2022/05/firefox-addons-for-2022/firefox-logo-collage-1.png title=/2022/05/firefox-addons-for-2022/firefox-logo-collage-1.png height=200 width=400></a></div><h2 class=related-title><a href=../../../2022/05/firefox-addons-for-2022/>Firefox Addons for 2022</a></h2></div><div class=related-item-container><div class=related-image><a href=../../../2021/07/goodbye-evernote-hello-bear/><img class=lazyload data-src=../../../2021/07/goodbye-evernote-hello-bear/bear_header-mac-screenshot.png data-srcset="../../../2021/07/goodbye-evernote-hello-bear/bear_header-mac-screenshot.png, ../../../2021/07/goodbye-evernote-hello-bear/bear_header-mac-screenshot.png 1.5x, ../../../2021/07/goodbye-evernote-hello-bear/bear_header-mac-screenshot.png 2x" data-sizes=auto alt=/2021/07/goodbye-evernote-hello-bear/bear_header-mac-screenshot.png title=/2021/07/goodbye-evernote-hello-bear/bear_header-mac-screenshot.png height=200 width=400></a></div><h2 class=related-title><a href=../../../2021/07/goodbye-evernote-hello-bear/>Goodbye Evernote, Hello Bear</a></h2></div><div class=related-item-container><div class=related-image><a href=../../../2020/10/the-best-of-2020-edition/><img class=lazyload data-src=../../../2020/10/the-best-of-2020-edition/william-hook-unsplash.jpg data-srcset="../../../2020/10/the-best-of-2020-edition/william-hook-unsplash.jpg, ../../../2020/10/the-best-of-2020-edition/william-hook-unsplash.jpg 1.5x, ../../../2020/10/the-best-of-2020-edition/william-hook-unsplash.jpg 2x" data-sizes=auto alt=/2020/10/the-best-of-2020-edition/william-hook-unsplash.jpg title=/2020/10/the-best-of-2020-edition/william-hook-unsplash.jpg height=200 width=400></a></div><h2 class=related-title><a href=../../../2020/10/the-best-of-2020-edition/>The Best Of - 2020 Edition</a></h2></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-10-31&nbsp;<a class=git-hash href=https://github.com/sammcj/smcleod-hugo/commit/28c2c547888f2ba4282f805d20e3bc774697430d target=_blank title="commit by Sam McLeod(sammcj@users.noreply.github.com) 28c2c547888f2ba4282f805d20e3bc774697430d: A new start" rel="noopener noreferrer">
<i class="fas fa-hashtag fa-fw"></i>28c2c54</a></span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-mardown href=../../../2017/09/glusterfs/index.md target=_blank rel="noopener noreferrer">Read markdown</a>
</span><span>|&nbsp;<a class=link-to-edit href=https://github.com/sammcj/smcleod-hugo/edit/main/content/posts/2017-09-25-GlusterFS-3-12-1/index.md target=_blank rel="noopener noreferrer">Edit this page</a></span></div><div class=post-info-share><span><a href=# onclick=return!1 title="Share on Twitter" data-sharer=twitter data-url=https://beta.smcleod.net/2017/09/glusterfs/ data-title=GlusterFS data-via=s_mcleod data-hashtags=tech,storage,software><i class="fab fa-twitter fa-fw"></i></a><a href=# onclick=return!1 title="Share on Linkedin" data-sharer=linkedin data-url=https://beta.smcleod.net/2017/09/glusterfs/><i class="fab fa-linkedin fa-fw"></i></a><a href=# onclick=return!1 title="Share on Hacker News" data-sharer=hackernews data-url=https://beta.smcleod.net/2017/09/glusterfs/ data-title=GlusterFS><i class="fab fa-hacker-news fa-fw"></i></a><a href=# onclick=return!1 title="Share on Reddit" data-sharer=reddit data-url=https://beta.smcleod.net/2017/09/glusterfs/><i class="fab fa-reddit fa-fw"></i></a><a href=# onclick=return!1 title="Share on Pocket" data-sharer=pocket data-url=https://beta.smcleod.net/2017/09/glusterfs/><i class="fab fa-get-pocket fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=../../../tags/tech/>tech</a>,&nbsp;<a href=../../../tags/storage/>storage</a>,&nbsp;<a href=../../../tags/software/>software</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=../../../>Home</a></span></section></div><div class=post-nav><a href=../../../2017/09/return-of-the-rss/ class=prev rel=prev title="Return Of The RSS"><i class="fas fa-angle-left fa-fw"></i>Return Of The RSS</a>
<a href=../../../2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet/ class=next rel=next title="Broadcom, Or How I Learned To Start Worrying And Drop The Packet">Broadcom, Or How I Learned To Start Worrying And Drop The Packet<i class="fas fa-angle-right fa-fw"></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2010 - 2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/sammcj target=_blank rel="noopener noreferrer">Sam McLeod</a></span></div><div class=footer-line></div><div class=footer-line></div></div></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=../../../atkinson-hyperlegible/atkinson-hyperlegible.eab28ad9966784d2ce9271970fa9cde34de4a323c0e4cd22fb5ffbf5261b049f52e236f1c06684c0ba73f72454f180c6e9e4667bee6cec7e9d5ac2d9dcb7b24e.css integrity="sha512-6rKK2ZZnhNLOknGXD6nN403koyPA5M0i+1/79SYbBJ9S4jbxwGaEwLpz9yRU8YDG6eRme+5s7H6dWsLZ3LeyTg=="><noscript><link rel=stylesheet href=../../../atkinson-hyperlegible/atkinson-hyperlegible.eab28ad9966784d2ce9271970fa9cde34de4a323c0e4cd22fb5ffbf5261b049f52e236f1c06684c0ba73f72454f180c6e9e4667bee6cec7e9d5ac2d9dcb7b24e.css integrity="sha512-6rKK2ZZnhNLOknGXD6nN403koyPA5M0i+1/79SYbBJ9S4jbxwGaEwLpz9yRU8YDG6eRme+5s7H6dWsLZ3LeyTg=="></noscript><script type=text/javascript src=../../../lib/autocomplete/autocomplete.min.0dcc8ee301dccbd9fb18115a1176e1293ca233b99aee5f5d12f218463c34eccea29f842827fc1ce330cf26e1e21b1382725ad3d0703cbde9f7e28552bcd92c07.js integrity="sha512-DcyO4wHcy9n7GBFaEXbhKTyiM7ma7l9dEvIYRjw07M6in4QoJ/wc4zDPJuHiGxOCclrT0HA8ven34oVSvNksBw=="></script><script type=text/javascript src=../../../lib/fuse/fuse.min.3dd1499ec34f39e57a4363bb184754a5023e0f9c47115d67dbec47058d78fb562f0c929c84d44a0a1ca7f84ed73b907e40fa84995338a66677721853139fb70f.js integrity="sha512-PdFJnsNPOeV6Q2O7GEdUpQI+D5xHEV1n2+xHBY14+1YvDJKchNRKChyn+E7XO5B+QPqEmVM4pmZ3chhTE5+3Dw=="></script><script type=text/javascript src=../../../lib/lazysizes/lazysizes.min.ab9f37a692ab09173b3793b49f69f352227eb2e52fec4b752467a5b386d739a30541c6a63e4f478dd5249d9bae16304db3bb6c32e69d81ee64f51cdd98efb519.js integrity="sha512-q583ppKrCRc7N5O0n2nzUiJ+suUv7Et1JGels4bXOaMFQcamPk9HjdUknZuuFjBNs7tsMuadge5k9RzdmO+1GQ=="></script><script type=text/javascript src=../../../lib/topbar/topbar.min.db5613fde0d014e29123106077225a764bdb4bdf791c2d252c1db5c09f19ea0bd181d29e10a1c03fddc6b0666d83ed17645ee84c31fb405fc7c0b8b38fd95eb0.js integrity="sha512-21YT/eDQFOKRIxBgdyJadkvbS995HC0lLB21wJ8Z6gvRgdKeEKHAP93GsGZtg+0XZF7oTDH7QF/HwLizj9lesA=="></script><script type=text/javascript src=../../../lib/pjax/pjax.min.8f92ba6fd6079d25aa09e1be3b1e3568438eba0fa53a093eb559684245a9c227d65e346122eb048ee9439ac9af9ce653c1ed88f20305c6121e946035c42705b0.js integrity="sha512-j5K6b9YHnSWqCeG+Ox41aEOOug+lOgk+tVloQkWpwifWXjRhIusEjulDmsmvnOZTwe2I8gMFxhIelGA1xCcFsA=="></script><script type=text/javascript src=../../../js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:100},comment:{},search:{distance:100,findAllMatches:!0,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:3,noResultsFound:"No results found",snippetLength:30,threshold:.3,type:"fuse",useExtendedSearch:!0},sharerjs:!0}</script><script type=text/javascript src=../../../lib/clipboard/clipboard.min.b08a94127467df50609e03e61edd897a7ce57830ec5f060efa2caf438e8cc3a44bfae7405198f69ffbbf3c663cd0dc51c729ceed1f71206c792c4cf0e0835625.js integrity="sha512-sIqUEnRn31BgngPmHt2JenzleDDsXwYO+iyvQ46Mw6RL+udAUZj2n/u/PGY80NxRxynO7R9xIGx5LEzw4INWJQ=="></script><script type=text/javascript src=../../../lib/sharer/sharer.min.cab91d53e60c836a90e16a5091059006925296eeed2c665fc744b7e7755ec3ab5c4e58e3c83899af70d8f690e004c2ca62062e11b0c7efdced87f80d074cfa3e.js integrity="sha512-yrkdU+YMg2qQ4WpQkQWQBpJSlu7tLGZfx0S353Vew6tcTljjyDiZr3DY9pDgBMLKYgYuEbDH79zth/gNB0z6Pg=="></script></div></body></html>