<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>storage - Tag - smcleod.net</title><link>https://beta.smcleod.net/tags/storage/</link><description>storage - Tag - smcleod.net</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Sam McLeod</copyright><lastBuildDate>Tue, 20 Mar 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://beta.smcleod.net/tags/storage/" rel="self" type="application/rss+xml"/><item><title>Flash Storage and SSD Failure Rate Update (March 2018)</title><link>https://beta.smcleod.net/2018/03/flash-storage-and-ssd-failure-rate-update-march-2018/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2018/03/flash-storage-and-ssd-failure-rate-update-march-2018/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2018/03/flash-storage-and-ssd-failure-rate-update-march-2018/sam-standing-at-rack.jpg" referrerpolicy="no-referrer">
            </div>It was almost 3 years ago that my open source storage project went into production. In that time it&rsquo;s been running 24/7 serving as highly available solid state storage for hundreds of VMs and several virtualisation clusters across our two main sites.
I&rsquo;m happy to report that the clusters have been operating very successfully since their conception.
Since moving away from proprietary &lsquo;black box&rsquo; vendor SANs, we haven&rsquo;t had a single SAN issue, storage outage.
We&rsquo;ve also been so please with the success of the design that we&rsquo;ve grown from a single cluster at each site to a total of 6 clusters (12 nodes).]]></description></item><item><title>Talk - Clustered, Distributed File and Volume Storage with GlusterFS</title><link>https://beta.smcleod.net/2017/11/talk-clustered-distributed-file-and-volume-storage-with-glusterfs/</link><pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2017/11/talk-clustered-distributed-file-and-volume-storage-with-glusterfs/</guid><description>&lt;div class="featured-image">
&lt;img src="/2017/11/talk-clustered-distributed-file-and-volume-storage-with-glusterfs/backdrop-plastic-containers.jpg" referrerpolicy="no-referrer">
&lt;/div>Using GlusterFS to provide volume storage to Kubernetes as a replacement for our existing file and static content hosting.
This talk was given at Infracoders on Tuesday 14th November 2017.
NOTE: Below link to slides currently broken - will fix soon! (03/08/2019)
Click below to view slides (PDF version): Direct download link</description></item><item><title>GlusterFS</title><link>https://beta.smcleod.net/2017/09/glusterfs/</link><pubDate>Mon, 25 Sep 2017 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2017/09/glusterfs/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2017/09/glusterfs/backdrop-insidebuilding.jpg" referrerpolicy="no-referrer">
            </div>We&rsquo;re in the process of shifting from using our custom &lsquo;glue&rsquo; for orchestrating Docker deployments to Kubernetes, When we first deployed Docker to replace LXC and our legacy Puppet-heavy application configuration and deployment systems there really wasn&rsquo;t any existing tool to manage this, thus we rolled our own, mainly a few Ruby scripts combined with a Puppet / Hiera / Mcollective driven workflow.
The main objective is to replace our legacy NFS file servers used to host uploads / attachments and static files for our web applications, while NFS(v4) performance is adequate, it is a clear single point of failure and of course, there are the age old stale mount problems should network interruptions occur.]]></description></item><item><title>Update Delayed Serial STONITH Design</title><link>https://beta.smcleod.net/2016/07/update-delayed-serial-stonith-design/</link><pubDate>Mon, 04 Jul 2016 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2016/07/update-delayed-serial-stonith-design/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2016/07/update-delayed-serial-stonith-design/noah-kuhn-27481.jpg" referrerpolicy="no-referrer">
            </div>note: This is a follow up post from 2015-07-21-rcd-stonith
A Linux Cluster Base STONITH provider for use with modern Pacemaker clustersThis has since been accepted and merged into Fedora&rsquo;s code base and as such will make it&rsquo;s way to RHEL.
Source Code: Github Diptrace CAD Design: Github I have open sourced the CAD circuit design and made this available within this repo under CAD Design and Schematics Related RedHat Bug: https://bugzilla.redhat.com/show_bug.cgi?id=1240868 v1 vs v2/v3 versions of the rcd_serial STONITH systemThe v2/v3 cables include the following improvements:
Have a connector on the outside of the server (that&rsquo;s female side runs from the reset pin &lsquo;hijacker&rsquo;) so that cables can be easily disconnected.]]></description></item><item><title>Benchmarking IO with FIO</title><link>https://beta.smcleod.net/2016/04/benchmarking-io-with-fio/</link><pubDate>Fri, 29 Apr 2016 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2016/04/benchmarking-io-with-fio/</guid><description>&lt;div class="featured-image">
&lt;img src="/2016/04/benchmarking-io-with-fio/daniel-mayovskiy-303666.jpg" referrerpolicy="no-referrer">
&lt;/div>This is a quick tldr there are many other situations and options you could consider FIO man page IOP/s = Input or Output operations per second Throughput = How many MB/s can you read/write continuously Variables worth tuning based on your situation --iodepth The iodepth is very dependant on your hardware.
Rotational drives without much cache and high latency (i.e. desktop SATA drives) will not benefit from a large iodepth, Values between 16 to 64 could be sensible.
High speed, lower latency SSDs (especially NVMe devices) can utilise a much higher iodepth, Values between 256 to 4096 could be sensible.</description></item><item><title>Fix XenServer SR with corrupt or invalid metadata</title><link>https://beta.smcleod.net/2016/01/fix-xenserver-sr-with-corrupt-or-invalid-metadata/</link><pubDate>Mon, 18 Jan 2016 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2016/01/fix-xenserver-sr-with-corrupt-or-invalid-metadata/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2016/01/fix-xenserver-sr-with-corrupt-or-invalid-metadata/samuel-zeller-358865.jpg" referrerpolicy="no-referrer">
            </div>If a disk / VDI is orphaned or only partially deleted you&rsquo;ll notice that under the SR it&rsquo;s not assigned to any VM.
This can cause issues that look like metadata corruption resulting in the inability to migrate VMs or edit storage.
For example:
[root@xenserver-host ~]# xe vdi-destroy uuid=6c2cd848-ac0e-441c-9cd6-9865fca7fe8b Error code: SR_BACKEND_FAILURE_181 Error parameters: , Error in Metadata volume operation for SR. [opterr=VDI delete operation failed for parameters: /dev/VG_XenStorage-3ae1df17-06ee-7202-eb92-72c266134e16/MGT, 6c2cd848-ac0e-441c-9cd6-9865fca7fe8b. Error: Failed to write file with params [3, 0, 512, 512]. Error: 5], Removing stale VDIsTo fix this, you need to remove those VDIs from the SR after first deleting the logical volume:]]></description></item><item><title>iSCSI SCSI-ID / Serial Persistence</title><link>https://beta.smcleod.net/2015/12/iscsi-scsi-id-/-serial-persistence/</link><pubDate>Mon, 14 Dec 2015 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2015/12/iscsi-scsi-id-/-serial-persistence/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2015/12/iscsi-scsi-id-/-serial-persistence/feature-san-fran.jpg" referrerpolicy="no-referrer">
            </div>&ldquo;Having a SCSI ID is a f*cking idiotic thing to do.&rdquo;
- Linus Torvalds
&hellip;and after the amount of time I&rsquo;ve wasted getting XenServer to play nicely with LIO iSCSI failover I tend to agree.
The Problem One oddity of Xen / XenServer&rsquo;s storage subsystem is that it identifies iSCSI storage repositories via a calculated SCSI ID rather than the iSCSI Serial - which would be the sane thing to do.
Citrix&rsquo;s less than ideal take on dealing with SCSI ID changes is for you to take your VMs offline, disconnected the storage repositories, recreate them, then go through all your VMs and re-attach their orphaned disks hoping that you remembered to add some sort of hint as to what VM they belong to, then finally wipe the sweat and tears from your face.]]></description></item><item><title>How to cluster and failover (almost) anything - An intro to Pacemaker and Corosync</title><link>https://beta.smcleod.net/2015/11/how-to-cluster-and-failover-almost-anything-an-intro-to-pacemaker-and-corosync/</link><pubDate>Mon, 09 Nov 2015 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2015/11/how-to-cluster-and-failover-almost-anything-an-intro-to-pacemaker-and-corosync/</guid><description>&lt;div class="featured-image">
&lt;img src="/2015/11/how-to-cluster-and-failover-almost-anything-an-intro-to-pacemaker-and-corosync/tim-napier-181584.jpg" referrerpolicy="no-referrer">
&lt;/div> Slides Failover Demo</description></item><item><title>SAN Intro</title><link>https://beta.smcleod.net/2015/10/san-intro/</link><pubDate>Wed, 07 Oct 2015 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2015/10/san-intro/</guid><description>&lt;div class="featured-image">
&lt;img src="/2015/10/san-intro/elliot-sloman-199291.jpg" referrerpolicy="no-referrer">
&lt;/div></description></item><item><title>SSD Storage - Two Months In Production</title><link>https://beta.smcleod.net/2015/09/ssd-storage-two-months-in-production/</link><pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2015/09/ssd-storage-two-months-in-production/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2015/09/ssd-storage-two-months-in-production/containers-1.jpg" referrerpolicy="no-referrer">
            </div>Over the last two months I&rsquo;ve been running selected IO intensive servers off the the SSD storage cluster, these hosts include (among others) our:
Primary Puppetmaster Gitlab server Redmine app and database servers Nagios servers Several Docker database host servers ReliabilityWe haven&rsquo;t had any software or hardware failures since commissioning the storage units.
During this time we have had 3 disk failures on our HP StoreVirtual SANs that have required us to call the supporting vendor and replace failed disks.
We have performed a great deal of live cluster failovers without any noticeable interruption to services and with no unexpected results.]]></description></item></channel></rss>