<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>hardware - Tag - smcleod.net</title><link>https://beta.smcleod.net/tags/hardware/</link><description>hardware - Tag - smcleod.net</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Sam McLeod</copyright><lastBuildDate>Sun, 11 Jul 2021 14:00:00 +0000</lastBuildDate><atom:link href="https://beta.smcleod.net/tags/hardware/" rel="self" type="application/rss+xml"/><item><title>Rancilio Silvia Upgrade</title><link>https://beta.smcleod.net/2021/07/rancilio-silvia-upgrade/</link><pubDate>Sun, 11 Jul 2021 14:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2021/07/rancilio-silvia-upgrade/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2021/07/rancilio-silvia-upgrade/assembled_preview.jpeg" referrerpolicy="no-referrer">
            </div>Weekend upgrades to my Ranchilio Silvia v4 espresso machine.
Added a digital PID kit for improved temperature control Replaced the boiler element Thermal insulation Basic &ldquo;smart&rdquo; power control with Siri integration Upgrade the standard Ranchilio basket to a VST 18g Ridged Rebuild PhotosNote the damage to the boiler elements terminals, this was shorting and causing the houses RCD to trip
Wiring the PID controller
Replacing the boiler thermostat with one connected to the PID controller
Adding insulation to the boiler
Before After ]]></description></item><item><title>Flash Storage and SSD Failure Rate Update (March 2018)</title><link>https://beta.smcleod.net/2018/03/flash-storage-and-ssd-failure-rate-update-march-2018/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2018/03/flash-storage-and-ssd-failure-rate-update-march-2018/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2018/03/flash-storage-and-ssd-failure-rate-update-march-2018/sam-standing-at-rack.jpg" referrerpolicy="no-referrer">
            </div>It was almost 3 years ago that my open source storage project went into production. In that time it&rsquo;s been running 24/7 serving as highly available solid state storage for hundreds of VMs and several virtualisation clusters across our two main sites.
I&rsquo;m happy to report that the clusters have been operating very successfully since their conception.
Since moving away from proprietary &lsquo;black box&rsquo; vendor SANs, we haven&rsquo;t had a single SAN issue, storage outage.
We&rsquo;ve also been so please with the success of the design that we&rsquo;ve grown from a single cluster at each site to a total of 6 clusters (12 nodes).]]></description></item><item><title>HP 4951C Protocol Analyser</title><link>https://beta.smcleod.net/2017/12/hp-4951c-protocol-analyser/</link><pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2017/12/hp-4951c-protocol-analyser/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2017/12/hp-4951c-protocol-analyser/backdrop-hp4951.jpg" referrerpolicy="no-referrer">
            </div>My good friend Joel Shea received a most unlikely gift this Christmas - A vintage HP 4951 Protocol Analyser.
According to the HP Computer Museum:
Original Price: $3595 The 4951B was replaced by the 4951C and 4952A in 1986. Both new models handled Async, BSC, SDLC, HDLC, X.25 and SNA protocols. The 4951C also handled DDCMP, while the 4952A did not. The 4952A handled X.21 while the 4951C did not. Both new analysers used a floppy dive (618 KB) for removable media.
Promo Photo:
Joel&rsquo;s specimen:
And the original spec sheet:]]></description></item><item><title>Broadcom, Or How I Learned To Start Worrying And Drop The Packet</title><link>https://beta.smcleod.net/2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet/</link><pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2017/10/broadcom-or-how-i-learned-to-start-worrying-and-drop-the-packet/office-space-broadcom.jpg" referrerpolicy="no-referrer">
            </div>Earlier this week we started the process to upgrade one of our hypervisor compute clusters when we encountered a rather painful bug with HP&rsquo;s Broadcom NIC chipsets.
We were part way through a routine rolling pool upgrade of our hypervisor (XenServer) cluster when we observed unexpected and intermittent loss of connectivity between several VMs, then entire XenServer hosts.
The problems appeared to impact hosts that hadn&rsquo;t yet upgraded to XenServer 7.2. We now attribute this to a symptom of extreme packet loss between the hosts in the pool and thanks to buggy firmware from Broadcom and HP.
We were aware of the recently published issues with Broadcom/HP NICs used in VMware clusters where NICs would be bricked by a firmware upgrade.]]></description></item><item><title>The State of Android in 2016 &amp; The OnePlus 3 Phone</title><link>https://beta.smcleod.net/2016/07/the-state-of-android-in-2016-the-oneplus-3-phone/</link><pubDate>Mon, 11 Jul 2016 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2016/07/the-state-of-android-in-2016-the-oneplus-3-phone/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2016/07/the-state-of-android-in-2016-the-oneplus-3-phone/adrien-296945.jpg" referrerpolicy="no-referrer">
            </div>I wanted to try Android for a couple of weeks, I like staying on top of technology, gadgets and making sure I never become a blind &lsquo;zealot&rsquo; for any platform or brand.
The OnePlus 3I did a lot of research and decided to try the &ldquo;Oneplus 3&rdquo; as it was good bang-for-buck, ran the latest software had plenty of grunt with the latest 8 core, high clock speed Qualcomm processor coupled with 6GB of DDR4 - the specs really are very impressive, especially for a $400USD phone.
Hardware wise the unit is lovely, not as high build quality as my iPhone 6s+ but not nearly as bad as the Samsung Galaxy S3 or other Samsung devices I had tried in the past.]]></description></item><item><title>Update Delayed Serial STONITH Design</title><link>https://beta.smcleod.net/2016/07/update-delayed-serial-stonith-design/</link><pubDate>Mon, 04 Jul 2016 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2016/07/update-delayed-serial-stonith-design/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2016/07/update-delayed-serial-stonith-design/noah-kuhn-27481.jpg" referrerpolicy="no-referrer">
            </div>note: This is a follow up post from 2015-07-21-rcd-stonith
A Linux Cluster Base STONITH provider for use with modern Pacemaker clustersThis has since been accepted and merged into Fedora&rsquo;s code base and as such will make it&rsquo;s way to RHEL.
Source Code: Github Diptrace CAD Design: Github I have open sourced the CAD circuit design and made this available within this repo under CAD Design and Schematics Related RedHat Bug: https://bugzilla.redhat.com/show_bug.cgi?id=1240868 v1 vs v2/v3 versions of the rcd_serial STONITH systemThe v2/v3 cables include the following improvements:
Have a connector on the outside of the server (that&rsquo;s female side runs from the reset pin &lsquo;hijacker&rsquo;) so that cables can be easily disconnected.]]></description></item><item><title>SAN Intro</title><link>https://beta.smcleod.net/2015/10/san-intro/</link><pubDate>Wed, 07 Oct 2015 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2015/10/san-intro/</guid><description>&lt;div class="featured-image">
&lt;img src="/2015/10/san-intro/elliot-sloman-199291.jpg" referrerpolicy="no-referrer">
&lt;/div></description></item><item><title>SSD Storage - Two Months In Production</title><link>https://beta.smcleod.net/2015/09/ssd-storage-two-months-in-production/</link><pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2015/09/ssd-storage-two-months-in-production/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2015/09/ssd-storage-two-months-in-production/containers-1.jpg" referrerpolicy="no-referrer">
            </div>Over the last two months I&rsquo;ve been running selected IO intensive servers off the the SSD storage cluster, these hosts include (among others) our:
Primary Puppetmaster Gitlab server Redmine app and database servers Nagios servers Several Docker database host servers ReliabilityWe haven&rsquo;t had any software or hardware failures since commissioning the storage units.
During this time we have had 3 disk failures on our HP StoreVirtual SANs that have required us to call the supporting vendor and replace failed disks.
We have performed a great deal of live cluster failovers without any noticeable interruption to services and with no unexpected results.]]></description></item><item><title>iSCSI Benchmarking</title><link>https://beta.smcleod.net/2015/07/iscsi-benchmarking/</link><pubDate>Fri, 24 Jul 2015 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2015/07/iscsi-benchmarking/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2015/07/iscsi-benchmarking/datacentre-1.jpg" referrerpolicy="no-referrer">
            </div>The following are benchmarks from our testings of our iSCSI SSD storage.
67,300 read IOP/s on a VM on iSCSI (Disk -&gt; LVM -&gt; MDADM -&gt; DRBD -&gt; iSCSI target -&gt; Network -&gt; XenServer iSCSI Client -&gt; VM) Per VM and scales to 1,000,000 IOP/s total root@dev-samm:/mnt/pmt1 128 # fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=128 --size=2G --readwrite=read test: (g=0): rw=read, bs=4K-4K/4K-4K, ioengine=libaio, iodepth=128 2.0.8 Starting 1 process bs: 1 (f=1): [R] [55.6% done] [262.1M/0K /s] [67.3K/0 iops] [eta 00m:04s] 38,500 random 4k write IOP/s on a VM on iSCSI (Disk -&gt; LVM -&gt; MDADM -&gt; DRBD -&gt; iSCSI target -&gt; Network -&gt; XenServer iSCSI Client -&gt; VM) Per VM and scales to 700,000 IOP/s total root@dev-samm:/mnt/pmt1 # fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=128 --size=2G --readwrite=randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K, ioengine=libaio, iodepth=128 2.]]></description></item><item><title>Delayed Serial STONITH</title><link>https://beta.smcleod.net/2015/07/delayed-serial-stonith/</link><pubDate>Tue, 21 Jul 2015 00:00:00 +0000</pubDate><author><name>Sam McLeod</name></author><guid>https://beta.smcleod.net/2015/07/delayed-serial-stonith/</guid><description><![CDATA[<div class="featured-image">
                <img src="/2015/07/delayed-serial-stonith/padlock.jpg" referrerpolicy="no-referrer">
            </div>A modified version of John Sutton&rsquo;s rcd_serial cable coupled with our Supermicro reset switch hijacker:
This works with the rcd_serial fence agent plugin.
Reasons rcd_serial makes for a very good STONITH mechanism:
It has no dependency on power state. It has no dependency on network state. It has no dependency on node operational state. It has no dependency on external hardware. It costs less that $5 + time to build. It is incredibly simple and reliable. Essentially the most common STONITH agent type in use is probably those that control UPS / PDUs, while this sounds like a good idea in theory there are a number of issues with relying on a UPS / PDU:]]></description></item></channel></rss>