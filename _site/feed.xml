<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">smcleod.net</title>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://smcleod.net/feed.xml" />
<link rel="alternate" type="text/html" href="http://smcleod.net" />
<updated>2015-02-16T12:17:05+11:00</updated>
<id>http://smcleod.net/</id>
<author>
  <name>Sam McLeod</name>
  <uri>http://smcleod.net/</uri>
  <email>smj(at)fastmail(dot)com</email>
</author>


<entry>
  <title type="html"><![CDATA[Building a high performance SSD SAN - Part 1]]></title>
 <link rel="alternate" type="text/html" href="http://smcleod.net/building-a-high-performance-ssd-san/" />
  <id>http://smcleod.net/building-a-high-performance-ssd-san</id>
  <published>2015-02-16T00:00:00+11:00</published>
  <updated>2015-02-16T00:00:00+11:00</updated>
  <author>
    <name>Sam McLeod</name>
    <uri>http://smcleod.net</uri>
    <email>smj(at)fastmail(dot)com</email>
  </author>
  <content type="html">
    &lt;p&gt;Over the coming month I will be architecting, building and testing a modular, high performance SSD-only storage solution.
I’ll be documenting my progress / findings along the way and open sourcing all the information as a public guide.&lt;/p&gt;

&lt;p&gt;With recent price drops and durability improvements in solid state storage now is better time than any to ditch those old magnets.
Modular server manufacturers such as SuperMicro have spent large on R&amp;amp;D thanks to the ever growing requirements from cloud vendors that utilise their hardware.&lt;/p&gt;

&lt;h2 id=&quot;the-state-of-enterprise-storage&quot;&gt;The State Of Enterprise Storage&lt;/h2&gt;

&lt;p&gt;Companies often settle for off-the-shelf large name storage products from companies based on several, often misguided assumptions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;That enterprise in the product name = reliability&lt;/li&gt;
  &lt;li&gt;That the blame from product / system failure can be outsourced&lt;/li&gt;
  &lt;li&gt;That vendors provide specialist engineers to support such complicated (and expensive) products&lt;/li&gt;
  &lt;li&gt;That that time and cost of building a modular storage solution tailored to their needs would be time consuming to design and manage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the end of the day we don’t trust vendors to design our servers - why would we trust them to design our storage?&lt;/p&gt;

&lt;p&gt;A great quote on Wikipedia under ‘enterprise storage’:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“You might think that the hardware inside a &lt;a href=&quot;http://en.m.wikipedia.org/wiki/Storage_area_network&quot; title=&quot;Storage area network&quot;&gt;SAN&lt;/a&gt; is vastly superior to what can be found in your average server, but that is not the case. EMC (the market leader) and others have disclosed more than once that “the goal has always to been to use as much standard, commercial, off-the-shelf hardware as we can”. So your SAN array is probably nothing more than a typical Xeon server built by Quanta with a shiny bezel. A decent professional 1 &lt;a href=&quot;http://en.m.wikipedia.org/wiki/Terabyte&quot; title=&quot;Terabyte&quot;&gt;TB&lt;/a&gt; drive costs a few hundred dollars. Place that same drive inside a SAN appliance and suddenly the price per terabyte is multiplied by at least three, sometimes even 10! When it comes to pricing and &lt;a href=&quot;http://en.m.wikipedia.org/wiki/Vendor_lock-in&quot; title=&quot;Vendor lock-in&quot;&gt;vendor lock-in&lt;/a&gt; you can say that storage systems are still stuck in the “mainframe era” despite the use of cheap off-the-shelf hardware.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It’s the same old story, if you’ve got lots of money and you don’t care about how you spend it or translating those savings onto your customers - sure &lt;em&gt;buy the ticket, take the ride&lt;/em&gt; - get a unit that comes with a flash logo, a 500 page brochure, licensing requirements and a greasy sales pitch.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;our-needs&quot;&gt;Our Needs&lt;/h2&gt;

&lt;p&gt;Storage performance always seems to be our bottleneck at Infoxchange, we run several high-performance high-concurrency applications with large databases and complex reporting.&lt;/p&gt;

&lt;p&gt;We’re grown (very) fast and with that spending too much on off-the-shelf storage solutions, we have a requirement to self-host most of our products securely within our own control, on our hardware and need to be flexible to meet current and emerging security requirements.&lt;/p&gt;

&lt;p&gt;I have been working on various proof-of-concepts which have lead to our decision to proceed with our own modular storage system tailored to our requirements.&lt;/p&gt;

&lt;h2 id=&quot;requirements&quot;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Reliability above all else
    &lt;ul&gt;
      &lt;li&gt;SSD units must be durable&lt;/li&gt;
      &lt;li&gt;Network and iSCSI failover must be on-par with commercial products (if not better)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multiple levels of provable redundancy
    &lt;ul&gt;
      &lt;li&gt;RAID&lt;/li&gt;
      &lt;li&gt;Cross hardware-replication&lt;/li&gt;
      &lt;li&gt;Easy IP and iSCSI failover using standard tools&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;1RU rack hight per unit&lt;/li&gt;
  &lt;li&gt;100% SSD only - no spindles will be hurt in the making of this journey!&lt;/li&gt;
  &lt;li&gt;Each unit to provide up to 450,000 IOP/s read performance on tier 1 storage&lt;/li&gt;
  &lt;li&gt;Provide up to 2.5GB/s read performance and 1.5GB/s write performance on tier 1 storage&lt;/li&gt;
  &lt;li&gt;Each unit to provide up to 400,000 IOP/s read performance on tier 2 storage&lt;/li&gt;
  &lt;li&gt;Provide up to 1.2GB/s read performance and 1.2GB/s write performance on tier 2 storage&lt;/li&gt;
  &lt;li&gt;20Gbit of redundant network connectivity per unit&lt;/li&gt;
  &lt;li&gt;Two tiers of SSD storage performance (PCIe &amp;amp; SATA)&lt;/li&gt;
  &lt;li&gt;Easily monitorable with standard tools&lt;/li&gt;
  &lt;li&gt;Use no proprietary RAID hardware&lt;/li&gt;
  &lt;li&gt;Come with 3 years of hardware warranty cover&lt;/li&gt;
  &lt;li&gt;Outperform all proprietary storage solutions costing twice the price or more&lt;/li&gt;
  &lt;li&gt;Deployable and manageable by any sysadmin and require no specialised storage administrators&lt;/li&gt;
  &lt;li&gt;Easily updatable for the latest security patches, features etc…&lt;/li&gt;
  &lt;li&gt;Highly customisable and easily upgradable to larger / faster storage in the future&lt;/li&gt;
  &lt;li&gt;Require significantly less energy and cooling over traditional storage units&lt;/li&gt;
  &lt;li&gt;Offer at-rest encryption if required&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;strong&gt;Cost less than $9.5K USD per node&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;software&quot;&gt;Software&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Operating System&lt;/td&gt;
      &lt;td&gt;Debian&lt;/td&gt;
      &lt;td&gt;Debian is our OS of choice, it has newer packages than RedHat variants and is incredibly stable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RAID&lt;/td&gt;
      &lt;td&gt;MDADM&lt;/td&gt;
      &lt;td&gt;For SSDs hardware RAID cards can often be their undoing - they simply can’t keep up and quickly become the bottleneck in the system. MDADM is mature and very flexible&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Node-to-Node Replication&lt;/td&gt;
      &lt;td&gt;DRBD&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NIC Bonding&lt;/td&gt;
      &lt;td&gt;LACP&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IP Failover&lt;/td&gt;
      &lt;td&gt;Pacemaker&lt;/td&gt;
      &lt;td&gt;We’ll probably also use a standard VM somewhere on our storage network for quorum&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Monitoring&lt;/td&gt;
      &lt;td&gt;Nagios&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Storage Presentation&lt;/td&gt;
      &lt;td&gt;Open-iSCSI&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Kernel&lt;/td&gt;
      &lt;td&gt;Latest Stable (Currently 3.18.7)&lt;/td&gt;
      &lt;td&gt;Debian Backports currently has Kernel 3.16, however we do daily CI builds of the latest kernel stable source for certain servers and this may be a good use case for them due the SCSI bus bypass for NVMe introduced in 3.18+&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We’re going to start with a two node cluster, we want to keep rack usage to a minimum so I’m going to go with a high density 1RU build.&lt;/p&gt;

&lt;p&gt;The servers themselves don’t need to be particularly powerful which will help us keep the costs down. Easily the most expensive components are the 1.2TB PCIe SSDs - but the performance and durability of these units can’t be overlooked, we’re going to have a second performance tier constructed of high end SATA SSDs in RAID10. Of course if you wanted to reduce price further the PCIe SSDs could be left out or purchased at a later date.&lt;/p&gt;

&lt;h2 id=&quot;hardware&quot;&gt;Hardware&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Base Server&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;SuperMicro SuperServer 1028R-WTNRT&lt;/td&gt;
      &lt;td&gt;2x 10GbE, NVMe Support, Dual PSU, Dual SATA DOM Support, 3x PCIe, 10x SAS/SATA HDD Bays&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;2x Intel Xeon E5-2609 v3&lt;/td&gt;
      &lt;td&gt;We shouldn’t need a very high clock speed for our SAN, but it’s worth getting the newer v3 processor range for the sake of future proofing.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;32GB DDR4 2133Mhz&lt;/td&gt;
      &lt;td&gt;Again, we don’t need that much RAM, however it will be used for disk caching but 32GB should be more than enough and can be easily upgraded at a later date.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PCIe SSD&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;2x 1.2TB Intel SSD DC P3600 Series (With NVMe)&lt;/td&gt;
      &lt;td&gt;This is where the real money goes - the Intel DC P3600 and P3700 series really are top of the range, the critical thing to note is that they support NVMe which will greatly increase performance, they’re backed by a 5 year warranty, these will be configured in RAID-1 for redundancy.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;SATA SSD&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;8x SanDisk Extreme Pro SSD 480GB&lt;/td&gt;
      &lt;td&gt;The SanDisk Extreme Pro line is arguably the most reliable and highest performing SATA SSD on the market - backed by a 10 year warranty, these will be configured in RAID-10 for redundancy and performance.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;OS SSD&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;2x 16GB MLC DOM&lt;/td&gt;
      &lt;td&gt;We don’t need much space for the OS, just enough to keep vital logs and package updates, these will be configured in RAID-1 for redundancy.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/images/san/sm.jpg&quot; alt=&quot;SuperMicro SuperServer 1028R-WTNRT&quot; /&gt;
&lt;img src=&quot;/images/san/mobo.jpg&quot; alt=&quot;SuperMicro SuperServer 1028R-WTNRT - mobo&quot; /&gt;
&lt;img src=&quot;/images/san/intel.jpg&quot; alt=&quot;1.2TB Intel SSD DC P3600 Series&quot; /&gt;
&lt;img src=&quot;/images/san/dom.jpg&quot; alt=&quot;SuperMicro DOM&quot; /&gt;
&lt;img src=&quot;/images/san/ssd.jpg&quot; alt=&quot;SanDisk Extreme Pro SSD 480GB&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ahci-vs-nvme&quot;&gt;AHCI vs NVMe&lt;/h2&gt;

&lt;p&gt;NVMe is a relatively new technology which I’m very interested in making use of for these storage units.&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;http://en.wikipedia.org/wiki/NVM_Express&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“NVM Express has been designed from the ground up, capitalizing on the low latency and parallelism of PCI Express SSDs, and mirroring the parallelism of contemporary CPUs, platforms and applications. By allowing parallelism levels offered by SSDs to be fully utilized by host’s hardware and software, NVM Express brings various performance improvements.”&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;-&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;AHCI&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;NVMe&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Maximum queue depth&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1 command queue; 32 commands per queue&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;65536 queues; 65536 commands per queue&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Uncacheable register accesses (2000 cycles each)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;6 per non-queued command; 9 per queued command&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2 per command&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;MSI-X and interrupt steering&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;single interrupt; no steering&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2048 MSI-X interrupts&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Parallelism and multiple threads&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;requires synchronization lock to issue a command&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;no locking&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Efficiency for 4 KB commands&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;command parameters require two serialized host DRAM fetches&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;gets command parameters in one 64 Bytes fetch&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;nvme-and-the-linux-kernel&quot;&gt;NVMe and the Linux Kernel&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Intel published an NVM Express driver for Linux, It was merged into the Linux Kernel mainline on 19 March 2012, with the release of version 3.3 of the Linux kernel.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A scalable block layer for high-performance SSD storage, developed primarily by_ &lt;a href=&quot;http://en.wikipedia.org/wiki/Fusion-io&quot;&gt;_Fusion-io&lt;/a&gt;_ _engineers, was merged into the Linux kernel mainline in kernel version 3.13, released on 19 January 2014. This leverages the performance offered by SSDs and NVM Express, by allowing much higher I/O submission rates. With this new design of the Linux kernel block layer, internal queues are split into two levels (per-CPU and hardware-submission queues), thus removing bottlenecks and allowing much higher levels of I/O parallelisation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note the following: &lt;em&gt;As of version 3.18 of the Linux kernel, released on 7 December 2014,&lt;/em&gt; [&lt;em&gt;VirtIO]&lt;a href=&quot;http://en.wikipedia.org/wiki/VirtIO&quot;&gt;6&lt;/a&gt;&lt;/em&gt; &lt;em&gt;block driver and the&lt;/em&gt; [&lt;em&gt;SCSI]&lt;a href=&quot;http://en.wikipedia.org/wiki/SCSI&quot;&gt;7&lt;/a&gt;__layer (which is used by Serial ATA drivers) have been modified to actually use this new interface; other drivers will be ported in the following releases.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Debian - our operating system of choice currently has kernel 3.16 available (using the official backports mirrors), however we do generate CI builds of the latest stable kernel for specific platforms - if you’re interested on how we’re doing that I have some information &lt;a href=&quot;http://smcleod.net/continuous-integration-for-the-linux-kernel-built-within-docker&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;That’s where I’m upto for now, the hardware will hopefully arrive in two weeks and I’ll begin the setup and testing.&lt;/p&gt;

&lt;h3 id=&quot;coming-soon&quot;&gt;Coming soon&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Build experience / guide&lt;/li&gt;
  &lt;li&gt;Monitoring&lt;/li&gt;
  &lt;li&gt;Benchmarks&lt;/li&gt;
  &lt;li&gt;Failover configuration and testing&lt;/li&gt;
  &lt;li&gt;Software configurations (Including a Puppet module)&lt;/li&gt;
  &lt;li&gt;Ongoing experiences and application&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;

&lt;p&gt;[&lt;em&gt;6/2/2015 - Sam McLeod]&lt;/em&gt;&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;http://smcleod.net/building-a-high-performance-ssd-san/&quot;&gt;Building a high performance SSD SAN - Part 1&lt;/a&gt; was originally published by Sam McLeod at &lt;a href=&quot;http://smcleod.net&quot;&gt;smcleod.net&lt;/a&gt; on February 16, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[XenServer, SSDs & VM Storage Performance]]></title>
 <link rel="alternate" type="text/html" href="http://smcleod.net/xenserver-ssds--vm-storage-performance/" />
  <id>http://smcleod.net/xenserver-ssds--vm-storage-performance</id>
  <published>2015-02-15T00:00:00+11:00</published>
  <updated>2015-02-15T00:00:00+11:00</updated>
  <author>
    <name>Sam McLeod</name>
    <uri>http://smcleod.net</uri>
    <email>smj(at)fastmail(dot)com</email>
  </author>
  <content type="html">
    &lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;At Infoxchange we use XenServer as our Virtualisation of choice.
There are many reasons for this including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open Source.&lt;/li&gt;
  &lt;li&gt;Offers greater performance than VMware.&lt;/li&gt;
  &lt;li&gt;Affordability (it’s free unless you purchase support).&lt;/li&gt;
  &lt;li&gt;Proven backend Xen is very reliable.&lt;/li&gt;
  &lt;li&gt;Reliable cross-host migrations of VMs.&lt;/li&gt;
  &lt;li&gt;The XenCentre client, (although having to run in a Windows VM) is quick and simple to use.&lt;/li&gt;
  &lt;li&gt;Upgrades and patches have proven to be more reliable than VMware.&lt;/li&gt;
  &lt;li&gt;OpenStack while interesting, is not yet reliable or streamlined enough for our small team of 4 to implement and manage.&lt;/li&gt;
  &lt;li&gt;XenServer Storage &amp;amp; Filesystems&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unfortunately the downside to XenServer is that it’s underlying OS is quite old.
The latest version (6.5) about to be released is still based on Centos 5 and still lacks any form of EXT4 and BTRFS support, direct disk access is not available… without some tweaking and has no real support for TRIM unless you have direct disk access and are happy with EXT3.&lt;/p&gt;

&lt;p&gt;Despite this, XenServer still manages to easily outperform VMware in both storage and CPU performance while costing… nothing unless you purchase support!&lt;/p&gt;

&lt;h2 id=&quot;direct-disk-access&quot;&gt;Direct disk access&lt;/h2&gt;

&lt;p&gt;It turns out, that you can add custom udev rules to pass through devices directly to VMs.&lt;/p&gt;

&lt;p&gt;For example, assuming &lt;code&gt;/dev/sdb&lt;/code&gt; is the disk you wish to make available to guests, you can edit &lt;code&gt;/etc/udev/rules.d/50-udev.rules&lt;/code&gt; like so:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;add&amp;quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;KERNEL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;sdb&amp;quot;&lt;/span&gt;, SYMLINK+&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;xapi/block/%k&amp;quot;&lt;/span&gt;, RUN+&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/bin/sh -c &amp;#39;/opt/xensource/libexec/local-device-change %k 2&amp;gt;&amp;amp;1 &amp;gt;/dev/null&amp;amp;&amp;#39;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;remove&amp;quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;KERNEL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;sdb&amp;quot;&lt;/span&gt;, RUN+&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/bin/sh -c &amp;#39;/opt/xensource/libexec/local-device-change %k 2&amp;gt;&amp;amp;1 &amp;gt;/dev/null&amp;amp;&amp;#39;&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There are some limitations with this:&lt;/p&gt;

&lt;p&gt;You’re passing through the whole disk and can only grant that entire disk to a single VM.
The udev configuration is volitile and is likely to be lost upon installing XenServer updates.
You cannot (to my knowledge) boot from directly attached storage devices.
What I’ve actually done is partitioned the SSD RAID array into 4 paritions on the XenServer host, this allows me to carve up the SSD RAID array and present adiquit storage to several VMs on the host.
i.e.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;add&amp;quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;KERNEL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;sdb1&amp;quot;&lt;/span&gt;, SYMLINK+&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;xapi/block/%k&amp;quot;&lt;/span&gt;, RUN+&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/bin/sh -c &amp;#39;/opt/xensource/libexec/local-device-change %k 2&amp;gt;&amp;amp;1 &amp;gt;/dev/null&amp;amp;&amp;#39;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;remove&amp;quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;KERNEL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;sdb1&amp;quot;&lt;/span&gt;, RUN+&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/bin/sh -c &amp;#39;/opt/xensource/libexec/local-device-change %k 2&amp;gt;&amp;amp;1 &amp;gt;/dev/null&amp;amp;&amp;#39;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;add&amp;quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;KERNEL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;sdb2&amp;quot;&lt;/span&gt;, SYMLINK+&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;xapi/block/%k&amp;quot;&lt;/span&gt;, RUN+&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/bin/sh -c &amp;#39;/opt/xensource/libexec/local-device-change %k 2&amp;gt;&amp;amp;1 &amp;gt;/dev/null&amp;amp;&amp;#39;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ACTION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;remove&amp;quot;&lt;/span&gt;, &lt;span class=&quot;nv&quot;&gt;KERNEL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;sdb2&amp;quot;&lt;/span&gt;, RUN+&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/bin/sh -c &amp;#39;/opt/xensource/libexec/local-device-change %k 2&amp;gt;&amp;amp;1 &amp;gt;/dev/null&amp;amp;&amp;#39;&amp;quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;etc…&lt;/p&gt;

&lt;p&gt;I’ve then:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mapped each partition to a VM running on that host.&lt;/li&gt;
  &lt;li&gt;Partitioned it within the host as needed.&lt;/li&gt;
  &lt;li&gt;Mounted /var/lib/docker as a BTRFS volume with compression enabled (compress=lzo) – This is used by CI as we build our our applications in Docker.&lt;/li&gt;
  &lt;li&gt;Mounted /home as an EXT4 – This is used by Gitlab-CI to checkout the pre-reqs for each build.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-benchmarks&quot;&gt;Performance (benchmarks)&lt;/h2&gt;

&lt;h3 id=&quot;observations&quot;&gt;Observations&lt;/h3&gt;

&lt;p&gt;The HP P410i RAID card on the old G6 DL360’s I am using for this is far underpowered and is unable to perform anywhere near the SSD’s rated speeds.
Direct disk access is 2-6 times ‘faster’ than XenServer’s standard LVM or EXT3 storage.
There is less than a 10% difference in performance between the SSD RAID array on the VM with the presented disk than directly on the XenServer host.
XenServer’s raw disk performance was exactly the same as Debian 7.
Remember to ensure your RAID (and disk cache if not in prod) is enabled!
Points of Note:&lt;/p&gt;

&lt;p&gt;This is an old server, we’re not talking your latest and greatest hardware here, we’re talking giving new life to an old-ish dog so that it may retain its usefulness while remaining cost effective.
With TRIM being unavailable when using RAID, it is expected that the write performance will decrease somewhat overtime as the disks fill up as the disks will have to perform a ‘READ, ERASE, WRITE’ rather than a simple WRITE, To aid the lack of TRIM, I have left more than 25% of the disks unused as we simply don’t need 1TB of SSD storage on each of the hosts.
We have some 1GB cache cards arriving in the following weeks which we will upgrade to from the 512MB cards presently installed – I expect this to significantly further improve performance.&lt;/p&gt;

&lt;h2 id=&quot;hardware&quot;&gt;Hardware:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Refurbished HP DL360 G6 (2010 model).&lt;/li&gt;
  &lt;li&gt;2x 8 Core Xeon x5550 w/ hyperthreading, 8 cores presented to domU.&lt;/li&gt;
  &lt;li&gt;96GB DDR3 in dom0, 4GB in domU.&lt;/li&gt;
  &lt;li&gt;HP P410i w/ 512MB cache.&lt;/li&gt;
  &lt;li&gt;2x 10K 146GB spindles for dom0.&lt;/li&gt;
  &lt;li&gt;‘HP Genuine’!&lt;/li&gt;
  &lt;li&gt;Constant 5-6 Watts each + cooling.&lt;/li&gt;
  &lt;li&gt;2x SanDisk Extreme Pro SSD 480GB in RAID 0&lt;/li&gt;
  &lt;li&gt;SATA III, Read up to 550MB/s, Write up to 515MB/s, Random Read up to 100K IOPS, Random Write up to 90K IOPS.&lt;/li&gt;
  &lt;li&gt;0.15-0.2 Watts peak each.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;EXT4 Bonnie++ Results w/ 2x SSD in RAID1, XenServer 6.5 RC1, LVM:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/xenserver/dl360-lvm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;EXT4 Bonnie++ Results w/ 2x SSD in RAID1, XenServer 6.5 RC1, Direct Disk:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/xenserver/dl360-dd.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;EXT4 dd Results w/ 2x SSD in RAID0, XenServer 6.5 RC1, LVM:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;samm@serv-dhcp-13:/var/tmp# dd &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/zero &lt;span class=&quot;nv&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;tempfile &lt;span class=&quot;nv&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1M &lt;span class=&quot;nv&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8000&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;fdatasync,notrunc
 8000+0 records in
 8000+0 records out
 &lt;span class=&quot;m&quot;&gt;8388608000&lt;/span&gt; bytes &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;8.4 GB&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; copied, 118.11 s, 71.0 MB/s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;EXT4 dd Results w/ 2x SSD in RAID0, XenServer 6.5 RC1, Direct Disk:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;samm@serv-dhcp-13:/var/tmp# dd &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/zero &lt;span class=&quot;nv&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;tempfile &lt;span class=&quot;nv&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1M &lt;span class=&quot;nv&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8000&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;fdatasync,notrunc
 8000+0 records in
 8000+0 records out
 &lt;span class=&quot;m&quot;&gt;8388608000&lt;/span&gt; bytes &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;8.4 GB&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; copied, 19.21 s, &lt;span class=&quot;m&quot;&gt;437&lt;/span&gt; MB/s&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Future Benchmarking Steps&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Observe performance over time since TRIM is not available.&lt;/li&gt;
  &lt;li&gt;Upgrade RAID cards to 1GB cache (we have some spares).&lt;/li&gt;
  &lt;li&gt;Try with our new Gen8 BL460c blade servers, with locally attached P420i RAID controllers and 2GB cache.&lt;/li&gt;
  &lt;li&gt;Try linux software RAID with direct disks.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;xenservers-future&quot;&gt;XenServer’s Future&lt;/h2&gt;

&lt;p&gt;Filesystems&lt;/p&gt;

&lt;p&gt;I would hope that both EXT4 (and BTRFS) support is added in the near future.
With this I would expect auto-detecting TRIM support similar to VMware and Hyper-V.
Direct Disk Access&lt;/p&gt;

&lt;p&gt;Direct disk access clearly offers massive performance gains, I would like to see XenServer add this as an easy to use option when configuring storage.
Non-volitile advanced configuration&lt;/p&gt;

&lt;p&gt;Related to direct disk access, XenServer needs some form of non-volitle advanced configuration options.
VMware, raw Xen and KVM let you tweak many options without risk of loss after installing minor updates.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://smcleod.net/xenserver-ssds--vm-storage-performance/&quot;&gt;XenServer, SSDs &amp; VM Storage Performance&lt;/a&gt; was originally published by Sam McLeod at &lt;a href=&quot;http://smcleod.net&quot;&gt;smcleod.net&lt;/a&gt; on February 15, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[The Best Of - 2014 Edition]]></title>
 <link rel="alternate" type="text/html" href="http://smcleod.net/the-best-of--2014-edition/" />
  <id>http://smcleod.net/the-best-of--2014-edition</id>
  <published>2015-02-15T00:00:00+11:00</published>
  <updated>2015-02-15T00:00:00+11:00</updated>
  <author>
    <name>Sam McLeod</name>
    <uri>http://smcleod.net</uri>
    <email>smj(at)fastmail(dot)com</email>
  </author>
  <content type="html">
    &lt;p&gt;At the end of every year I note down a summary of the best applications, hardware &amp;amp; websites I’ve enjoyed &amp;amp; depended on throughout the year (and often for some time before).&lt;/p&gt;

&lt;h2 id=&quot;software--general-use&quot;&gt;Software / General Use:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Fastmail - https://www.fastmail.com&lt;/li&gt;
  &lt;li&gt;Evernote - https://evernote.com&lt;/li&gt;
  &lt;li&gt;Reeder - http://reederapp.com&lt;/li&gt;
  &lt;li&gt;Keynote - https://www.apple.com/au/mac/keynote&lt;/li&gt;
  &lt;li&gt;Lastpass - https://lastpass.com&lt;/li&gt;
  &lt;li&gt;Plex - https://plex.tv&lt;/li&gt;
  &lt;li&gt;Calibre - http://calibre-ebook.com
&lt;!--more--&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;software--geek-use&quot;&gt;Software / Geek Use:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Sublime Text - http://www.sublimetext.com/3&lt;/li&gt;
  &lt;li&gt;Homebrew - http://brew.sh&lt;/li&gt;
  &lt;li&gt;DropSync - http://mudflatsoftware.com&lt;/li&gt;
  &lt;li&gt;Beets - http://beets.radbox.org&lt;/li&gt;
  &lt;li&gt;Textual - http://www.codeux.com/textual&lt;/li&gt;
  &lt;li&gt;XLD - http://tmkk.undo.jp/xld/index_e.html&lt;/li&gt;
  &lt;li&gt;Code Academy - http://www.codecademy.com&lt;/li&gt;
  &lt;li&gt;Exercism.io - http://exercism.io&lt;/li&gt;
  &lt;li&gt;Sickbeard + Headphones + Couchpotato + Sabnzbd - http://www.totalhtpc.com/ultimate-usenet-guide.html&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;software--mobile&quot;&gt;Software / Mobile:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Tweetbot - http://tapbots.com/software/tweetbot&lt;/li&gt;
  &lt;li&gt;Pushover - http://pushover.net&lt;/li&gt;
  &lt;li&gt;Lastpass - https://lastpass.com&lt;/li&gt;
  &lt;li&gt;Keynote - https://www.apple.com/au/ios/keynote&lt;/li&gt;
  &lt;li&gt;Reeder - http://reederapp.com/ios&lt;/li&gt;
  &lt;li&gt;Evernote- https://evernote.com&lt;/li&gt;
  &lt;li&gt;Plex - https://plex.tv&lt;/li&gt;
  &lt;li&gt;Backblaze - https://www.backblaze.com&lt;/li&gt;
  &lt;li&gt;WTF Podcast - http://www.wtfpod.com/app&lt;/li&gt;
  &lt;li&gt;MiniHack - https://itunes.apple.com/au/app/minihack-for-hacker-news/id631108846?mt=8&lt;/li&gt;
  &lt;li&gt;Uber - https://itunes.apple.com/au/app/uber/id368677368?mt=8&lt;/li&gt;
  &lt;li&gt;Goodreads - https://itunes.apple.com/au/app/goodreads-book-recommendations/id355833469?mt=8&lt;/li&gt;
  &lt;li&gt;Notify4M - https://itunes.apple.com/au/app/notify4m/id499161979?mt=8&lt;/li&gt;
  &lt;li&gt;Bandcamp - https://itunes.apple.com/au/app/bandcamp/id706408639?mt=8&lt;/li&gt;
  &lt;li&gt;Hype Machine - https://itunes.apple.com/au/app/hype-machine/id414315986?mt=8&lt;/li&gt;
  &lt;li&gt;Nuzzel (Only got onto this today) - https://itunes.apple.com/au/app/nuzzel-news-from-your-friends/id692285770?mt=8&lt;/li&gt;
  &lt;li&gt;Alien Blue - http://www.reddit.com/r/alienblue&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;software--sysadmin-or-devops-specific&quot;&gt;Software / SysAdmin or DevOps Specific:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Gitlab - http://gitlab.org&lt;/li&gt;
  &lt;li&gt;Gitlab-CI - https://about.gitlab.com/gitlab-ci&lt;/li&gt;
  &lt;li&gt;Dash - http://kapeli.com/dash&lt;/li&gt;
  &lt;li&gt;PostgreSQL (Makes my list every year &amp;amp; just keeps getting better) - http://www.postgresql.org&lt;/li&gt;
  &lt;li&gt;PGBadger - http://dalibo.github.io/pgbadger&lt;/li&gt;
  &lt;li&gt;Docker - https://www.docker.com&lt;/li&gt;
  &lt;li&gt;Consul - https://consul.io&lt;/li&gt;
  &lt;li&gt;Puppet (I couldn’t do my job as well without it) - http://puppetlabs.com&lt;/li&gt;
  &lt;li&gt;iTerm - http://iterm2.com&lt;/li&gt;
  &lt;li&gt;Nginx - http://nginx.org&lt;/li&gt;
  &lt;li&gt;Htop - http://hisham.hm/htop&lt;/li&gt;
  &lt;li&gt;Bonnie++ - http://linux.die.net/man/8/bonnie++&lt;/li&gt;
  &lt;li&gt;Openfire - https://www.igniterealtime.org/projects/openfire&lt;/li&gt;
  &lt;li&gt;Hiera-Eyaml - https://github.com/TomPoulton/hiera-eyaml&lt;/li&gt;
  &lt;li&gt;Rubinius - http://rubini.us&lt;/li&gt;
  &lt;li&gt;Puma - http://puma.io&lt;/li&gt;
  &lt;li&gt;XenServer - http://xenserver.org&lt;/li&gt;
  &lt;li&gt;ElasticSearch - www.elasticsearch.org&lt;/li&gt;
  &lt;li&gt;Logstash - http://logstash.net&lt;/li&gt;
  &lt;li&gt;FPM - https://github.com/jordansissel/fpm&lt;/li&gt;
  &lt;li&gt;PFsense - https://www.pfsense.org&lt;/li&gt;
  &lt;li&gt;Debian Jessie (Not quite released but the next great version of the best linux Distro IMO) - https://www.debian.org/releases/jessie&lt;/li&gt;
  &lt;li&gt;Check_MK Multisite - https://mathias-kettner.de/checkmk_multisite.html&lt;/li&gt;
  &lt;li&gt;PWSafe - https://itunes.apple.com/au/app/pwsafe-password-safe-compatible/id520993579?mt=12&lt;/li&gt;
  &lt;li&gt;Supervisord - http://supervisord.org&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;websites&quot;&gt;Websites:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Last.fm - http://www.last.fm/user/sammcj2000&lt;/li&gt;
  &lt;li&gt;Feedly - https://feedly.com&lt;/li&gt;
  &lt;li&gt;HackerNews - https://news.ycombinator.com/news&lt;/li&gt;
  &lt;li&gt;Lucidchart - http://lucidchart.com/&lt;/li&gt;
  &lt;li&gt;MondoTunes (Might be a little biased here!) - http://mondotunes.org&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hardware&quot;&gt;Hardware:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Das Ultimate 4- http://www.daskeyboard.com&lt;/li&gt;
  &lt;li&gt;Logitech Performance MX - http://www.logitech.com/en-au/product/performance-mouse-mx&lt;/li&gt;
  &lt;li&gt;15’’ Macbook Pro Retina - http://www.apple.com/au/macbook-pro&lt;/li&gt;
  &lt;li&gt;iPhone 6+ (because its bigger than bigger, or something) - https://www.apple.com/iphone-6&lt;/li&gt;
  &lt;li&gt;CuBox - http://www.solid-run.com/product/cubox-i4pro&lt;/li&gt;
  &lt;li&gt;Parani SD1000 Bluetooth Serial Adapter - http://www.senaindustrial.com/products/industrial_bluetooth/sd1000.php&lt;/li&gt;
  &lt;li&gt;SanDisk Extreme Pro 480GB SSD - http://www.newegg.com/Product/Product.aspx?Item=N82E16820171999&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other--non-tech&quot;&gt;Other / Non-tech:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;All Day Socks - http://alldaysocks.com&lt;/li&gt;
  &lt;li&gt;Sennheiser Amperior On-Ear Headphones - http://www.head-fi.org/products/sennheiser-amperior-on-ear-headphones&lt;/li&gt;
  &lt;li&gt;O2 + ODAC - http://www.jdslabs.com/products/48/o2-odac-combo&lt;/li&gt;
  &lt;li&gt;Benchmark DAC1 HDR - http://benchmarkmedia.com/products/benchmark-dac1-hdr-digital-to-analog-converter&lt;/li&gt;
  &lt;li&gt;Bellroy Wallets - http://bellroy.com&lt;/li&gt;
  &lt;li&gt;Ink Shoes - http://www.inkshoes.it&lt;/li&gt;
  &lt;li&gt;Kindle Paperwhite - http://www.amazon.com.au/gp/feature.html?docId=3077740006&lt;/li&gt;
  &lt;li&gt;Nerf Jolt - http://nerf.wikia.com/wiki/Jolt_EX-1&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;books&quot;&gt;Books:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The Phoenix Project (Reread this year for the 3rd time) - http://itrevolution.com/books/phoenix-project-devops-book&lt;/li&gt;
  &lt;li&gt;Surely You’re Joking Mr Feynman! - https://www.goodreads.com/book/show/5544.Surely_You_re_Joking_Mr_Feynman_&lt;/li&gt;
  &lt;li&gt;The Dark Tower Series - https://www.goodreads.com/book/show/43615.The_Gunslinger&lt;/li&gt;
  &lt;li&gt;Snow Crash - https://www.goodreads.com/book/show/830.Snow_Crash&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;http://smcleod.net/the-best-of--2014-edition/&quot;&gt;The Best Of - 2014 Edition&lt;/a&gt; was originally published by Sam McLeod at &lt;a href=&quot;http://smcleod.net&quot;&gt;smcleod.net&lt;/a&gt; on February 15, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Talk - 24 Months]]></title>
 <link rel="alternate" type="text/html" href="http://smcleod.net/talk-24-months/" />
  <id>http://smcleod.net/talk-24-months</id>
  <published>2015-02-15T00:00:00+11:00</published>
  <updated>2015-02-15T00:00:00+11:00</updated>
  <author>
    <name>Sam McLeod</name>
    <uri>http://smcleod.net</uri>
    <email>smj(at)fastmail(dot)com</email>
  </author>
  <content type="html">
    &lt;p&gt;The way we work at Infoxchange has changed greatly.&lt;/p&gt;

&lt;p&gt;A retrospective journey into transforming Infoxchange’s technology and culture over the past 24 months - presented a Melbourne DevOps – December 2014&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ixa.io/wp-content/uploads/2014/11/24%20Months.pdf&quot;&gt;&lt;img src=&quot;/images/misc/24months.jpg&quot; alt=&quot;Click to Start Slides&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://smcleod.net/talk-24-months/&quot;&gt;Talk - 24 Months&lt;/a&gt; was originally published by Sam McLeod at &lt;a href=&quot;http://smcleod.net&quot;&gt;smcleod.net&lt;/a&gt; on February 15, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Search – A Journey of Delivery on a Budget]]></title>
 <link rel="alternate" type="text/html" href="http://smcleod.net/search--a-journey-of-delivery-on-a-budget/" />
  <id>http://smcleod.net/search--a-journey-of-delivery-on-a-budget</id>
  <published>2015-02-15T00:00:00+11:00</published>
  <updated>2015-02-15T00:00:00+11:00</updated>
  <author>
    <name>Sam McLeod</name>
    <uri>http://smcleod.net</uri>
    <email>smj(at)fastmail(dot)com</email>
  </author>
  <content type="html">
    &lt;p&gt;Presented a Melbourne Search – July 2014 – ‘Search – A Journey of Delivery on a Budget’.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ixa.io/slides/search/index.html&quot;&gt;&lt;img src=&quot;/images/misc/search.jpg&quot; alt=&quot;Click to Start Slides&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://smcleod.net/search--a-journey-of-delivery-on-a-budget/&quot;&gt;Search – A Journey of Delivery on a Budget&lt;/a&gt; was originally published by Sam McLeod at &lt;a href=&quot;http://smcleod.net&quot;&gt;smcleod.net&lt;/a&gt; on February 15, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Limited Pagination in Jekyll with Liquid]]></title>
 <link rel="alternate" type="text/html" href="http://smcleod.net/limited-pagination-in-jekyll-with-liquid/" />
  <id>http://smcleod.net/limited-pagination-in-jekyll-with-liquid</id>
  <published>2015-02-15T00:00:00+11:00</published>
  <updated>2015-02-15T00:00:00+11:00</updated>
  <author>
    <name>Sam McLeod</name>
    <uri>http://smcleod.net</uri>
    <email>smj(at)fastmail(dot)com</email>
  </author>
  <content type="html">
    &lt;p&gt;I have another blog mondotunes.org which is a brain dump of whatever music I’m listening to and enjoying - I’ve recently (over night) switched from Wordpress to Jekyll.
Jekyll’s default pagination is to show a list of all pages on your blog, which is pretty intrusive if you have a lot of pages:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/misc/pagenationbad.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So we need to set a range to limit on:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/misc/pagenationdiff.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Much better:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/misc/pagenationgood.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://smcleod.net/limited-pagination-in-jekyll-with-liquid/&quot;&gt;Limited Pagination in Jekyll with Liquid&lt;/a&gt; was originally published by Sam McLeod at &lt;a href=&quot;http://smcleod.net&quot;&gt;smcleod.net&lt;/a&gt; on February 15, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Direct-Attach SSD Storage – Performance & Comparisons]]></title>
 <link rel="alternate" type="text/html" href="http://smcleod.net/directattach-ssd-storage--performance--comparisons/" />
  <id>http://smcleod.net/directattach-ssd-storage--performance--comparisons</id>
  <published>2015-02-15T00:00:00+11:00</published>
  <updated>2015-02-15T00:00:00+11:00</updated>
  <author>
    <name>Sam McLeod</name>
    <uri>http://smcleod.net</uri>
    <email>smj(at)fastmail(dot)com</email>
  </author>
  <content type="html">
    &lt;p&gt;Further to my earlier post on XenServer storage performance with regards to directly attaching storage from the host, I have been analysing the performance of various SSD storage options.
I have attached a HP DS2220sb storage blade to an existing server blade and compared performance with 4 and 6 SSD RAID-10 to our existing iSCSI SANs.&lt;/p&gt;

&lt;p&gt;While the P420i RAID controller in the DS2220sb is clearly saturated and unable to provide throughput much over 1,100MB/s – the IOP/s available to PostgreSQL are still a very considerably performance improvement over our P4530 SAN – in fact, 6 SSD’s result in a 39.9x performance increase!&lt;/p&gt;

&lt;p&gt;Click the image below for the results:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ixa.io/wp-content/uploads/2015/01/SSDvsSAN.pdf&quot;&gt;&lt;img src=&quot;/images/san/bladedirectattach.png&quot; alt=&quot;Click to Start Slides&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://smcleod.net/directattach-ssd-storage--performance--comparisons/&quot;&gt;Direct-Attach SSD Storage – Performance &amp; Comparisons&lt;/a&gt; was originally published by Sam McLeod at &lt;a href=&quot;http://smcleod.net&quot;&gt;smcleod.net&lt;/a&gt; on February 15, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Continuous Integration for the Linux Kernel - Built within Docker]]></title>
 <link rel="alternate" type="text/html" href="http://smcleod.net/continuous-integration-for-the-linux-kernel--built-within-docker/" />
  <id>http://smcleod.net/continuous-integration-for-the-linux-kernel--built-within-docker</id>
  <published>2015-02-15T00:00:00+11:00</published>
  <updated>2015-02-15T00:00:00+11:00</updated>
  <author>
    <name>Sam McLeod</name>
    <uri>http://smcleod.net</uri>
    <email>smj(at)fastmail(dot)com</email>
  </author>
  <content type="html">
    &lt;p&gt;Builds a Debian package of the latest stable linux kernel.&lt;/p&gt;

&lt;p&gt;Runs in an isolated and disposble docker container.
Tested with Gitlab-CI but should work on any CI system.
No root access is required for the build.&lt;/p&gt;

&lt;p&gt;After a successfully building the kernel package, the kernel will be copied to /mnt/storage on the host. Once the kernel package is on the host, you can install it on any Debian Wheezy AMD64 machine or upload it to your local apt repo for deployment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/sammcj/kernel-ci&quot;&gt;Code - https://github.com/sammcj/kernel-ci&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ci/kernelci.png&quot; alt=&quot;Gitlab-Kernel-CI&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://smcleod.net/continuous-integration-for-the-linux-kernel--built-within-docker/&quot;&gt;Continuous Integration for the Linux Kernel - Built within Docker&lt;/a&gt; was originally published by Sam McLeod at &lt;a href=&quot;http://smcleod.net&quot;&gt;smcleod.net&lt;/a&gt; on February 15, 2015.&lt;/p&gt;
  </content>
</entry>

</feed>
