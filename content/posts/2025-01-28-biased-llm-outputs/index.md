---
title: "Biased LLM Outputs, Tiananmen Square & Americanisations"
date: 2025-01-28T01:00:01+10:00
tags: ["ai","llm","tech","deepseek","llama","openai","anthropic","china","america","bias","facts"]
author: "Sam McLeod"
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
description: "Realising bias in LLMs is important, but it goes both ways."
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: false
mermaid: false
---

R̶e̶a̶l̶i̶z̶i̶n̶g̶ Realising bias in LLMs is important, but it goes both ways.

<!-- more -->

I see a vast number of people wasting a lot more of their time correcting the written English (spelling) in almost every single output generated from American trained LLMs - than I do correcting Chinese trained LLMs on what happened at Tiananmen Square.

Just as we need to have systems to augment and verify LLMs knowledge with facts - it would be pretty nice to not have to replace Zs with Ss in the output of every single models output.

I'm not equating Americaniz̶sation to revisionist history, I'm just pointing out that the most frequent example given does not have the most frequent real-world error rate.

---

For context - I just so often see comments on DeepSeek/Qwen etc... releases to the effect of "I don't trust this model to write code, it can't tell me what happened at Tiananmen Square, it's the CCP propaganda machine"!

All of those things are probably true to a certain extent but I think they usually miss the mark - while at the same time I'm constantly having to correct models from the USA in real world use cases because they can't spell. This time and effort is real, and it adds up.
